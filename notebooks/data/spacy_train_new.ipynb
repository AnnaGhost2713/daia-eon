{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-07-03T08:53:06.301489Z",
     "start_time": "2025-07-03T08:53:05.599538Z"
    }
   },
   "source": [
    "# ðŸ§© Schritt 1: Imports und Setup\n",
    "import spacy\n",
    "from spacy.tokens import DocBin\n",
    "from spacy.training.example import Example\n",
    "import json\n",
    "import random\n",
    "from pathlib import Path\n",
    "from spacy.util import minibatch\n",
    "\n",
    "# ðŸ“ Schritt 2: Funktion zum Laden von Daten aus JSON\n",
    "def load_data_from_json(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        raw_data = json.load(f)\n",
    "    if isinstance(raw_data, dict):\n",
    "        raw_data = [raw_data]\n",
    "\n",
    "    TRAIN_DATA = []\n",
    "    for entry in raw_data:\n",
    "        text = entry[\"text\"]\n",
    "        entities = [(label[\"start\"], label[\"end\"], label[\"label\"]) for label in entry[\"labels\"]]\n",
    "        TRAIN_DATA.append((text, {\"entities\": entities}))\n",
    "    return TRAIN_DATA\n",
    "\n",
    "# ðŸ”„ Lade Trainings- und Dev-Daten separat\n",
    "train_data = load_data_from_json(\"./spacy_split/train.json\")\n",
    "dev_data = load_data_from_json(\"./spacy_split/dev.json\")\n",
    "print(f\"ðŸ“¥ Trainingsbeispiele: {len(train_data)}, Dev-Beispiele: {len(dev_data)}\")\n",
    "\n",
    "# ðŸ§  Schritt 3: Lade spaCy-Basismodell\n",
    "base_model = \"de_core_news_md\"\n",
    "nlp = spacy.load(base_model)\n",
    "\n",
    "# Stelle sicher, dass NER-Komponente existiert\n",
    "if \"ner\" not in nlp.pipe_names:\n",
    "    ner = nlp.add_pipe(\"ner\", last=True)\n",
    "else:\n",
    "    ner = nlp.get_pipe(\"ner\")\n",
    "\n",
    "# Registriere alle Labels aus beiden DatensÃ¤tzen\n",
    "for dataset in (train_data, dev_data):\n",
    "    for _, annotations in dataset:\n",
    "        for start, end, label in annotations[\"entities\"]:\n",
    "            ner.add_label(label)\n",
    "\n",
    "# ðŸš€ Schritt 4: Modell-Initialisierung mit allen Daten (nur fÃ¼r Labels!)\n",
    "def get_examples():\n",
    "    for text, ann in train_data + dev_data:\n",
    "        yield Example.from_dict(nlp.make_doc(text), ann)\n",
    "\n",
    "optimizer = nlp.initialize(get_examples=get_examples)\n",
    "\n",
    "# ðŸ‹ï¸ Schritt 5: Training (nur auf Trainingsdaten)\n",
    "n_iter = 20\n",
    "for i in range(n_iter):\n",
    "    random.shuffle(train_data)\n",
    "    losses = {}\n",
    "\n",
    "    batches = minibatch(train_data, size=8)\n",
    "    for batch in batches:\n",
    "        examples = [Example.from_dict(nlp.make_doc(text), ann) for text, ann in batch]\n",
    "        nlp.update(examples, drop=0.35, losses=losses)\n",
    "\n",
    "    print(f\"ðŸ” Iteration {i+1}/{n_iter}, Loss: {losses['ner']:.4f}\")\n",
    "\n",
    "# ðŸ’¾ Schritt 6: Modell speichern\n",
    "output_dir = Path(\"custom_spacy_model_new\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "nlp.to_disk(output_dir)\n",
    "print(f\"\\nâœ… Modell gespeichert unter: {output_dir.resolve()}\")\n",
    "\n",
    "# ðŸ” Schritt 7: Modell laden und auf dev_data testen\n",
    "nlp2 = spacy.load(output_dir)\n",
    "\n",
    "print(\"\\nðŸ“Š Evaluation auf dev_data:\")\n",
    "for text, _ in random.sample(dev_data, min(5, len(dev_data))):  # max. 5 Beispiele\n",
    "    doc = nlp2(text)\n",
    "    print(f\"\\n> {text}\")\n",
    "    for ent in doc.ents:\n",
    "        print(f\"  - {ent.text} ({ent.label_})\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lade 112 Trainingsbeispiele.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "[E103] Trying to set conflicting doc.ents: '(130, 136, 'VORNAME')' and '(130, 136, 'FIRMA')'. A token can only be part of one entity, so make sure the entities you're setting don't overlap. To work with overlapping entities, consider using doc.spans instead.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[2], line 58\u001B[0m\n\u001B[1;32m     56\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m text, annotations \u001B[38;5;129;01min\u001B[39;00m train_data:\n\u001B[1;32m     57\u001B[0m     doc \u001B[38;5;241m=\u001B[39m nlp\u001B[38;5;241m.\u001B[39mmake_doc(text)\n\u001B[0;32m---> 58\u001B[0m     example \u001B[38;5;241m=\u001B[39m \u001B[43mExample\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_dict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdoc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mannotations\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     59\u001B[0m     nlp\u001B[38;5;241m.\u001B[39mupdate([example], drop\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.35\u001B[39m, losses\u001B[38;5;241m=\u001B[39mlosses)\n\u001B[1;32m     60\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIteration \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mi\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mn_iter\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, Losses: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mlosses\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m/opt/anaconda3/envs/piranha-env/lib/python3.10/site-packages/spacy/training/example.pyx:130\u001B[0m, in \u001B[0;36mspacy.training.example.Example.from_dict\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m/opt/anaconda3/envs/piranha-env/lib/python3.10/site-packages/spacy/training/example.pyx:34\u001B[0m, in \u001B[0;36mspacy.training.example.annotations_to_doc\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m/opt/anaconda3/envs/piranha-env/lib/python3.10/site-packages/spacy/training/example.pyx:511\u001B[0m, in \u001B[0;36mspacy.training.example._add_entities_to_doc\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m/opt/anaconda3/envs/piranha-env/lib/python3.10/site-packages/spacy/training/iob_utils.py:114\u001B[0m, in \u001B[0;36moffsets_to_biluo_tags\u001B[0;34m(doc, entities, missing)\u001B[0m\n\u001B[1;32m    112\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m token_index \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(start_char, end_char):\n\u001B[1;32m    113\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m token_index \u001B[38;5;129;01min\u001B[39;00m tokens_in_ents\u001B[38;5;241m.\u001B[39mkeys():\n\u001B[0;32m--> 114\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    115\u001B[0m             Errors\u001B[38;5;241m.\u001B[39mE103\u001B[38;5;241m.\u001B[39mformat(\n\u001B[1;32m    116\u001B[0m                 span1\u001B[38;5;241m=\u001B[39m(\n\u001B[1;32m    117\u001B[0m                     tokens_in_ents[token_index][\u001B[38;5;241m0\u001B[39m],\n\u001B[1;32m    118\u001B[0m                     tokens_in_ents[token_index][\u001B[38;5;241m1\u001B[39m],\n\u001B[1;32m    119\u001B[0m                     tokens_in_ents[token_index][\u001B[38;5;241m2\u001B[39m],\n\u001B[1;32m    120\u001B[0m                 ),\n\u001B[1;32m    121\u001B[0m                 span2\u001B[38;5;241m=\u001B[39m(start_char, end_char, label),\n\u001B[1;32m    122\u001B[0m             )\n\u001B[1;32m    123\u001B[0m         )\n\u001B[1;32m    124\u001B[0m     tokens_in_ents[token_index] \u001B[38;5;241m=\u001B[39m (start_char, end_char, label)\n\u001B[1;32m    125\u001B[0m start_token \u001B[38;5;241m=\u001B[39m starts\u001B[38;5;241m.\u001B[39mget(start_char)\n",
      "\u001B[0;31mValueError\u001B[0m: [E103] Trying to set conflicting doc.ents: '(130, 136, 'VORNAME')' and '(130, 136, 'FIRMA')'. A token can only be part of one entity, so make sure the entities you're setting don't overlap. To work with overlapping entities, consider using doc.spans instead."
     ]
    }
   ],
   "execution_count": 2
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
