{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e450e8-04ee-485c-b6fa-66e7051108a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/AnnaGhost2713/daia-eon.git\n",
    "%cd daia-eon/notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4db2af-a951-4fea-820e-e61850ccb9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### PREVIEW OF TXT FILES (WHETHER IT WORKS) ####\n",
    "\n",
    "# 0) install & imports\n",
    "!pip install -q transformers sentencepiece tqdm\n",
    "\n",
    "import re, time, random\n",
    "from math        import ceil\n",
    "from pathlib     import Path\n",
    "from random      import seed\n",
    "from collections import Counter\n",
    "from tqdm.auto   import tqdm\n",
    "from transformers import pipeline, set_seed\n",
    "\n",
    "# 1) CONFIG + load all .txt → records with spans\n",
    "DATA_DIR = Path(\"data/golden_dataset_anonymized_granular\")\n",
    "all_txt  = sorted(DATA_DIR.glob(\"*.txt\"))\n",
    "records  = []\n",
    "for f in all_txt:\n",
    "    txt = f.read_text(\"utf-8\")\n",
    "    labs = [{\"start\":m.start(),\"end\":m.end(),\"label\":m.group(1)}\n",
    "           for m in re.finditer(r\"<<([^>]+)>>\", txt)]\n",
    "    records.append({\"file\":f.name, \"text\":txt, \"labels\":labs})\n",
    "\n",
    "# 2) train/test split (same IDs as before)\n",
    "TEST_IDS   = {0,142,2,3,146,145,157,165,19,18,20,166,176,177,\n",
    "              32,34,40,45,52,57,61,65,66,70,71,73,75,78,81,\n",
    "              96,102,105,108,109,112,115,122,129,132,134}\n",
    "TEST_FILES = {f\"{i}.txt\" for i in TEST_IDS}\n",
    "train_recs = [r for r in records if r[\"file\"] not in TEST_FILES]\n",
    "\n",
    "# 3) lock & sample a tiny preview\n",
    "seed(1)\n",
    "preview = random.sample(train_recs, k=5)\n",
    "print(\"Previewing:\", [r[\"file\"] for r in preview])\n",
    "\n",
    "# 4) compute variant counts on that preview\n",
    "tag_counts = Counter(l[\"label\"] for r in preview for l in r[\"labels\"])\n",
    "max_cnt    = max(tag_counts.values(), default=1)\n",
    "def n_variants_for(r):\n",
    "    freqs = [tag_counts.get(l[\"label\"],1) for l in r[\"labels\"]]\n",
    "    return ceil(max_cnt / max(min(freqs),1)) if freqs else 1\n",
    "\n",
    "# 5) instantiate your de↔en back-translator\n",
    "# … (Schritte 0–4 wie gehabt) …\n",
    "\n",
    "# 5) viel freieres Sampling\n",
    "kw = dict(device=-1,\n",
    "          do_sample=True,\n",
    "          top_k=300,\n",
    "          top_p=0.95,\n",
    "          temperature=1.5)\n",
    "de_en = pipeline(\"translation_de_to_en\", model=\"Helsinki-NLP/opus-mt-de-en\", **kw)\n",
    "en_de = pipeline(\"translation_en_to_de\", model=\"Helsinki-NLP/opus-mt-en-de\", **kw)\n",
    "en_fr = pipeline(\"translation_en_to_fr\", model=\"Helsinki-NLP/opus-mt-en-fr\", **kw)\n",
    "fr_en = pipeline(\"translation_fr_to_en\", model=\"Helsinki-NLP/opus-mt-fr-en\", **kw)\n",
    "en_es = pipeline(\"translation_en_to_es\", model=\"Helsinki-NLP/opus-mt-en-es\", **kw)\n",
    "es_en = pipeline(\"translation_es_to_en\", model=\"Helsinki-NLP/opus-mt-es-en\", **kw)\n",
    "en_it = pipeline(\"translation_en_to_it\", model=\"Helsinki-NLP/opus-mt-en-it\", **kw)\n",
    "it_en = pipeline(\"translation_it_to_en\", model=\"Helsinki-NLP/opus-mt-it-en\", **kw)\n",
    "\n",
    "pivot_pipes = {\n",
    "  \"fr\": (en_fr, fr_en),\n",
    "  \"es\": (en_es, es_en),\n",
    "  \"it\": (en_it, it_en),\n",
    "}\n",
    "\n",
    "def bt_super_diverse(text: str, want: int) -> list[str]:\n",
    "    # 1) mask placeholders\n",
    "    tags, masked = [], text\n",
    "    for i, t in enumerate(re.findall(r\"(<<[^>]+>>)\", text), 1):\n",
    "        tags.append(t)\n",
    "        masked = masked.replace(t, f\"[TAG{i}]\")\n",
    "\n",
    "    # 2) deutsch→englisch (mehr Beams)\n",
    "    en_beams = de_en(\n",
    "      masked,\n",
    "      max_length=512, truncation=True,\n",
    "      num_beams=want*2,\n",
    "      num_return_sequences=want,\n",
    "      early_stopping=True\n",
    "    )\n",
    "    out_variants = []\n",
    "    for beam in en_beams:\n",
    "        en = beam[\"translation_text\"]\n",
    "        time.sleep(0.1)\n",
    "\n",
    "        # 3) zufällige Pivot-Hop-Logik\n",
    "        hop = random.random()\n",
    "        if hop < 0.3:\n",
    "            lang, (e2p, p2e) = random.choice(list(pivot_pipes.items()))\n",
    "            en = p2e(e2p(en, max_length=512, truncation=True)[0][\"translation_text\"],\n",
    "                     max_length=512, truncation=True)[0][\"translation_text\"]\n",
    "            time.sleep(0.2)\n",
    "        elif hop < 0.5:\n",
    "            # Zweifach-Hop DE→EN→FR→EN\n",
    "            mid = pivot_pipes[\"fr\"][0](en, max_length=512, truncation=True)[0][\"translation_text\"]\n",
    "            time.sleep(0.1)\n",
    "            en  = pivot_pipes[\"fr\"][1](mid, max_length=512, truncation=True)[0][\"translation_text\"]\n",
    "            time.sleep(0.1)\n",
    "\n",
    "        # 4) englisch→deutsch\n",
    "        de = en_de(en, max_length=512, truncation=True)[0][\"translation_text\"]\n",
    "        time.sleep(0.1)\n",
    "\n",
    "        # 5) unmask\n",
    "        for i, t in enumerate(tags, 1):\n",
    "            de = de.replace(f\"[TAG{i}]\", t)\n",
    "        out_variants.append(de)\n",
    "\n",
    "    return out_variants\n",
    "\n",
    "# 6) Preview\n",
    "for rec in tqdm(preview, desc=\"Super-Diverse Preview\"):\n",
    "    want = n_variants_for(rec)\n",
    "    print(f\"\\n→ {rec['file']} (need {want} variants)\")\n",
    "    for v in bt_super_diverse(rec[\"text\"], want):\n",
    "        print(\"  \", v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1729947f-06f1-4ae1-9d09-44141bc070fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### PREVIEW OF JSON FILE (WHETHER IT WORKS) #### \n",
    "\n",
    "# 0) install & imports\n",
    "!pip install -q transformers sentencepiece tqdm\n",
    "\n",
    "import re, time, random\n",
    "from math        import ceil\n",
    "from pathlib     import Path\n",
    "from random      import seed\n",
    "from collections import Counter\n",
    "from tqdm.auto   import tqdm\n",
    "from transformers import pipeline, set_seed\n",
    "import json\n",
    "\n",
    "# 1) CONFIG + load all .txt → records with spans\n",
    "DATA_DIR = Path(\"data/golden_dataset_anonymized_granular\")\n",
    "all_txt  = sorted(DATA_DIR.glob(\"*.txt\"))\n",
    "records  = []\n",
    "for f in all_txt:\n",
    "    txt = f.read_text(\"utf-8\")\n",
    "    labs = [{\"start\":m.start(),\"end\":m.end(),\"label\":m.group(1)}\n",
    "           for m in re.finditer(r\"<<([^>]+)>>\", txt)]\n",
    "    records.append({\"file\":f.name, \"text\":txt, \"labels\":labs})\n",
    "\n",
    "# 2) train/test split (same IDs as before)\n",
    "TEST_IDS   = {0,142,2,3,146,145,157,165,19,18,20,166,176,177,\n",
    "              32,34,40,45,52,57,61,65,66,70,71,73,75,78,81,\n",
    "              96,102,105,108,109,112,115,122,129,132,134}\n",
    "TEST_FILES = {f\"{i}.txt\" for i in TEST_IDS}\n",
    "train_recs = [r for r in records if r[\"file\"] not in TEST_FILES]\n",
    "\n",
    "# 3) lock & sample a tiny preview\n",
    "seed(1)\n",
    "preview = random.sample(train_recs, k=2)\n",
    "print(\"Previewing:\", [r[\"file\"] for r in preview])\n",
    "\n",
    "# 4) compute variant counts on that preview\n",
    "tag_counts = Counter(l[\"label\"] for r in preview for l in r[\"labels\"])\n",
    "max_cnt    = max(tag_counts.values(), default=1)\n",
    "def n_variants_for(r):\n",
    "    freqs = [tag_counts.get(l[\"label\"],1) for l in r[\"labels\"]]\n",
    "    return ceil(max_cnt / max(min(freqs),1)) if freqs else 1\n",
    "\n",
    "# 5) instantiate your de↔en back-translator\n",
    "# … (Schritte 0–4 wie gehabt) …\n",
    "\n",
    "# 5) viel freieres Sampling\n",
    "kw = dict(device=-1,\n",
    "          do_sample=True,\n",
    "          top_k=300,\n",
    "          top_p=0.95,\n",
    "          temperature=1.5)\n",
    "de_en = pipeline(\"translation_de_to_en\", model=\"Helsinki-NLP/opus-mt-de-en\", **kw)\n",
    "en_de = pipeline(\"translation_en_to_de\", model=\"Helsinki-NLP/opus-mt-en-de\", **kw)\n",
    "en_fr = pipeline(\"translation_en_to_fr\", model=\"Helsinki-NLP/opus-mt-en-fr\", **kw)\n",
    "fr_en = pipeline(\"translation_fr_to_en\", model=\"Helsinki-NLP/opus-mt-fr-en\", **kw)\n",
    "en_es = pipeline(\"translation_en_to_es\", model=\"Helsinki-NLP/opus-mt-en-es\", **kw)\n",
    "es_en = pipeline(\"translation_es_to_en\", model=\"Helsinki-NLP/opus-mt-es-en\", **kw)\n",
    "en_it = pipeline(\"translation_en_to_it\", model=\"Helsinki-NLP/opus-mt-en-it\", **kw)\n",
    "it_en = pipeline(\"translation_it_to_en\", model=\"Helsinki-NLP/opus-mt-it-en\", **kw)\n",
    "\n",
    "pivot_pipes = {\n",
    "  \"fr\": (en_fr, fr_en),\n",
    "  \"es\": (en_es, es_en),\n",
    "  \"it\": (en_it, it_en),\n",
    "}\n",
    "\n",
    "def bt_super_diverse(text: str, want: int) -> list[str]:\n",
    "    # 1) mask placeholders\n",
    "    tags, masked = [], text\n",
    "    for i, t in enumerate(re.findall(r\"(<<[^>]+>>)\", text), 1):\n",
    "        tags.append(t)\n",
    "        masked = masked.replace(t, f\"[TAG{i}]\")\n",
    "\n",
    "    # 2) deutsch→englisch (mehr Beams)\n",
    "    en_beams = de_en(\n",
    "      masked,\n",
    "      max_length=512, truncation=True,\n",
    "      num_beams=want*2,\n",
    "      num_return_sequences=want,\n",
    "      early_stopping=True\n",
    "    )\n",
    "    out_variants = []\n",
    "    for beam in en_beams:\n",
    "        en = beam[\"translation_text\"]\n",
    "        time.sleep(0.1)\n",
    "\n",
    "        # 3) zufällige Pivot-Hop-Logik\n",
    "        hop = random.random()\n",
    "        if hop < 0.3:\n",
    "            lang, (e2p, p2e) = random.choice(list(pivot_pipes.items()))\n",
    "            en = p2e(e2p(en, max_length=512, truncation=True)[0][\"translation_text\"],\n",
    "                     max_length=512, truncation=True)[0][\"translation_text\"]\n",
    "            time.sleep(0.2)\n",
    "        elif hop < 0.5:\n",
    "            # Zweifach-Hop DE→EN→FR→EN\n",
    "            mid = pivot_pipes[\"fr\"][0](en, max_length=512, truncation=True)[0][\"translation_text\"]\n",
    "            time.sleep(0.1)\n",
    "            en  = pivot_pipes[\"fr\"][1](mid, max_length=512, truncation=True)[0][\"translation_text\"]\n",
    "            time.sleep(0.1)\n",
    "\n",
    "        # 4) englisch→deutsch\n",
    "        de = en_de(en, max_length=512, truncation=True)[0][\"translation_text\"]\n",
    "        time.sleep(0.1)\n",
    "\n",
    "        # 5) unmask\n",
    "        for i, t in enumerate(tags, 1):\n",
    "            de = de.replace(f\"[TAG{i}]\", t)\n",
    "        out_variants.append(de)\n",
    "\n",
    "    return out_variants\n",
    "\n",
    "\n",
    "# 6) build & write preview JSON\n",
    "OUT_FILE = Path(\"data/preview_paraphrases.json\")\n",
    "results = []\n",
    "\n",
    "for rec in tqdm(preview, desc=\"Building preview JSON\"):\n",
    "    want     = n_variants_for(rec)\n",
    "    variants = bt_super_diverse(rec[\"text\"], want)\n",
    "    results.append({\n",
    "        \"file\":       rec[\"file\"],\n",
    "        \"n_variants\": want,\n",
    "        \"variants\":   variants\n",
    "    })\n",
    "\n",
    "with OUT_FILE.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"✓ Wrote preview to {OUT_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782837ad-d74a-47e6-a60e-3cb3e1f0fd08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "files.download('data/preview_paraphrases.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd69870-65e7-4553-a11d-f95eab1008b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### CODE FOR THE TOTAL OF 120 TRAINING MAILS ###\n",
    "# 0) install & imports\n",
    "!pip install -q transformers sentencepiece tqdm\n",
    "\n",
    "import re, time, random, json\n",
    "from math          import ceil\n",
    "from pathlib       import Path\n",
    "from random        import seed\n",
    "from collections   import Counter\n",
    "from tqdm.auto     import tqdm\n",
    "from transformers  import pipeline, set_seed\n",
    "\n",
    "# 1) CONFIG + load all .txt → records with spans\n",
    "DATA_DIR = Path(\"data/golden_dataset_anonymized_granular\")\n",
    "all_txt  = sorted(DATA_DIR.glob(\"*.txt\"))\n",
    "records  = []\n",
    "for f in all_txt:\n",
    "    txt = f.read_text(\"utf-8\")\n",
    "    labs = [{\"start\":m.start(),\"end\":m.end(),\"label\":m.group(1)}\n",
    "           for m in re.finditer(r\"<<([^>]+)>>\", txt)]\n",
    "    records.append({\"file\":f.name, \"text\":txt, \"labels\":labs})\n",
    "\n",
    "# 2) train/test split (same IDs as before)\n",
    "TEST_IDS   = {0,142,2,3,146,145,157,165,19,18,20,166,176,177,\n",
    "              32,34,40,45,52,57,61,65,66,70,71,73,75,78,81,\n",
    "              96,102,105,108,109,112,115,122,129,132,134}\n",
    "TEST_FILES = {f\"{i}.txt\" for i in TEST_IDS}\n",
    "train_recs = [r for r in records if r[\"file\"] not in TEST_FILES]\n",
    "\n",
    "# 3) compute tag‐frequency *on the full train set* (not just preview)\n",
    "tag_counts = Counter(l[\"label\"] for r in train_recs for l in r[\"labels\"])\n",
    "max_cnt    = max(tag_counts.values(), default=1)\n",
    "def n_variants_for(r):\n",
    "    freqs = [tag_counts.get(l[\"label\"],1) for l in r[\"labels\"]]\n",
    "    return ceil(max_cnt / max(min(freqs),1)) if freqs else 1\n",
    "\n",
    "# 4) instantiate all pipelines **on GPU** (device=0)\n",
    "kw = dict(device=0, do_sample=True, top_k=300, top_p=0.95, temperature=1.5)\n",
    "de_en = pipeline(\"translation_de_to_en\", model=\"Helsinki-NLP/opus-mt-de-en\", **kw)\n",
    "en_de = pipeline(\"translation_en_to_de\", model=\"Helsinki-NLP/opus-mt-en-de\", **kw)\n",
    "en_fr = pipeline(\"translation_en_to_fr\", model=\"Helsinki-NLP/opus-mt-en-fr\", **kw)\n",
    "fr_en = pipeline(\"translation_fr_to_en\", model=\"Helsinki-NLP/opus-mt-fr-en\", **kw)\n",
    "en_es = pipeline(\"translation_en_to_es\", model=\"Helsinki-NLP/opus-mt-en-es\", **kw)\n",
    "es_en = pipeline(\"translation_es_to_en\", model=\"Helsinki-NLP/opus-mt-es-en\", **kw)\n",
    "en_it = pipeline(\"translation_en_to_it\", model=\"Helsinki-NLP/opus-mt-en-it\", **kw)\n",
    "it_en = pipeline(\"translation_it_to_en\", model=\"Helsinki-NLP/opus-mt-it-en\", **kw)\n",
    "\n",
    "pivot_pipes = {\n",
    "    \"fr\": (en_fr, fr_en),\n",
    "    \"es\": (en_es, es_en),\n",
    "    \"it\": (en_it, it_en),\n",
    "}\n",
    "\n",
    "# 5) super‐diverse back‐translator (same as before)\n",
    "def bt_super_diverse(text: str, want: int) -> list[str]:\n",
    "    tags, masked = [], text\n",
    "    for i, t in enumerate(re.findall(r\"(<<[^>]+>>)\", text), 1):\n",
    "        tags.append(t)\n",
    "        masked = masked.replace(t, f\"[TAG{i}]\")\n",
    "\n",
    "    en_beams = de_en(\n",
    "        masked,\n",
    "        max_length=512, truncation=True,\n",
    "        num_beams=want*2,\n",
    "        num_return_sequences=want,\n",
    "        early_stopping=True\n",
    "    )\n",
    "    out_variants = []\n",
    "    for beam in en_beams:\n",
    "        en = beam[\"translation_text\"]\n",
    "        time.sleep(0.1)\n",
    "\n",
    "        hop = random.random()\n",
    "        if hop < 0.3:\n",
    "            lang, (e2p, p2e) = random.choice(list(pivot_pipes.items()))\n",
    "            en = p2e(\n",
    "                e2p(en, max_length=512, truncation=True)[0][\"translation_text\"],\n",
    "                max_length=512, truncation=True\n",
    "            )[0][\"translation_text\"]\n",
    "            time.sleep(0.2)\n",
    "        elif hop < 0.5:\n",
    "            mid = pivot_pipes[\"fr\"][0](en, max_length=512, truncation=True)[0][\"translation_text\"]\n",
    "            time.sleep(0.1)\n",
    "            en  = pivot_pipes[\"fr\"][1](mid, max_length=512, truncation=True)[0][\"translation_text\"]\n",
    "            time.sleep(0.1)\n",
    "\n",
    "        de = en_de(en, max_length=512, truncation=True)[0][\"translation_text\"]\n",
    "        time.sleep(0.1)\n",
    "\n",
    "        for i, t in enumerate(tags, 1):\n",
    "            de = de.replace(f\"[TAG{i}]\", t)\n",
    "        out_variants.append(de)\n",
    "\n",
    "    return out_variants\n",
    "\n",
    "# 6) build & write full JSON\n",
    "OUT_FILE = Path(\"data/option_a_paraphrases.json\")\n",
    "results  = []\n",
    "\n",
    "for rec in tqdm(train_recs, desc=\"Building full JSON\"):\n",
    "    want     = n_variants_for(rec)\n",
    "    variants = bt_super_diverse(rec[\"text\"], want)\n",
    "    results.append({\n",
    "        \"file\":       rec[\"file\"],\n",
    "        \"n_variants\": want,\n",
    "        \"variants\":   variants\n",
    "    })\n",
    "\n",
    "OUT_FILE.parent.mkdir(exist_ok=True, parents=True)\n",
    "with OUT_FILE.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"✓ Wrote all  paraphases to {OUT_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e74b861-a69b-4ffd-bd58-e26289547366",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "files.download('data/option_a_paraphrases.json')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
