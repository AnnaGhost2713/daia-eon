{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Gemini Email Anonymization Evaluation\n",
    "\n",
    "**What it does:** Evaluates the quality of Gemini-anonymized emails by aligning each placeholder (e.g. `<<VORNAME_2>>`) to its ground-truth span and computing precision, recall, and F1 at both label and file level.\n",
    "\n",
    "**Inputs:**\n",
    "- **Ground truth** — JSON list where each item contains `\"file\"` and a `\"labels\"` array (`start`, `end`, `label`).\n",
    "- **Predictions** — JSON list where each item contains `\"file\"` and `\"anonymized_text\"` with placeholders like `<<LABEL_3>>`.\n",
    "\n",
    "**Processing:**\n",
    "- Build per-file maps of ground-truth entities sorted by start offset.\n",
    "- Extract placeholders and map them to ground-truth entities via their ordinal index (`_1`, `_2`, …).\n",
    "- Count true positives, false positives, and false negatives per file and per label.\n",
    "- Aggregate micro- and macro-level precision, recall, and F1 metrics.\n",
    "\n",
    "**Output:**\n",
    "- Console report showing overall micro P/R/F1, macro F1, TP/FP/FN totals, plus per-label and per-file breakdowns.\n",
    "\n",
    "**Requirements:** Python 3.8+ (standard library only).\n",
    "\n",
    "**Example:**\n",
    "```bash\n",
    "# Adjust the hard-coded paths in the script (or add CLI args) and run:\n",
    "python llm_evaluation_new.py"
   ],
   "id": "22c54d37728eec97"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import json\n",
    "import re\n",
    "import sys\n",
    "from collections import defaultdict, Counter\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "\n",
    "Entity = Tuple[int, int, str]  # (start, end, label)\n",
    "\n",
    "\n",
    "def load_json(path: str):\n",
    "    with open(path, encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "\n",
    "def build_gt_mapping(ground_truth_records: List[Dict]) -> Dict[str, Dict[str, List[Entity]]]:\n",
    "    \"\"\"\n",
    "    Returns: { filename: { label: [ (start, end, label), ... sorted by start ] } }\n",
    "    \"\"\"\n",
    "    mapping = {}\n",
    "    for rec in ground_truth_records:\n",
    "        file = rec[\"file\"]\n",
    "        labels = rec.get(\"labels\", [])\n",
    "        per_label = defaultdict(list)\n",
    "        for ent in labels:\n",
    "            start, end, label = ent[\"start\"], ent[\"end\"], ent[\"label\"]\n",
    "            per_label[label].append((start, end, label))\n",
    "        for label in per_label:\n",
    "            per_label[label].sort(key=lambda x: x[0])\n",
    "        mapping[file] = per_label\n",
    "    return mapping\n",
    "\n",
    "\n",
    "def extract_predictions_from_anonymized(anonymized_records: List[Dict],\n",
    "                                       gt_mapping: Dict[str, Dict[str, List[Entity]]]\n",
    "                                      ) -> Dict[str, List[Entity]]:\n",
    "    \"\"\"\n",
    "    For each file, extract placeholders like <<LABEL_2>> and map them to spans in ground truth via ordinal.\n",
    "    Returns: { filename: [ (start, end, label), ... ] } predicted entities\n",
    "    \"\"\"\n",
    "    placeholder_re = re.compile(r\"<<([A-Z_]+)_(\\d+)>>\")\n",
    "    predictions = {}\n",
    "\n",
    "    for rec in anonymized_records:\n",
    "        file = rec[\"file\"]\n",
    "        text = rec.get(\"anonymized_text\", \"\")\n",
    "        preds = []\n",
    "        gt_labels_for_file = gt_mapping.get(file, {})\n",
    "        for match in placeholder_re.finditer(text):\n",
    "            label = match.group(1)\n",
    "            index = int(match.group(2))  # 1-based\n",
    "            gt_list = gt_labels_for_file.get(label, [])\n",
    "            if 1 <= index <= len(gt_list):\n",
    "                span = gt_list[index - 1]  # ordinal mapping\n",
    "                preds.append(span)\n",
    "            else:\n",
    "                # No corresponding ground truth span: sentinel to count as FP\n",
    "                preds.append((-1, -1, label))\n",
    "        predictions[file] = preds\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def compute_confusion_single_file(gt_labels: Dict[str, List[Entity]],\n",
    "                                  pred_entities: List[Entity]\n",
    "                                 ) -> Tuple[Counter, Counter, Counter]:\n",
    "    \"\"\"\n",
    "    Compute TP/FP/FN for one file.\n",
    "    \"\"\"\n",
    "    tp = Counter()\n",
    "    fp = Counter()\n",
    "    fn = Counter()\n",
    "\n",
    "    gt_entities_all = []\n",
    "    for label, lst in gt_labels.items():\n",
    "        gt_entities_all.extend(lst)\n",
    "    gt_set = set(gt_entities_all)\n",
    "    pred_set = set(pred_entities)\n",
    "\n",
    "    for ent in pred_set:\n",
    "        if ent in gt_set and ent[0] != -1:\n",
    "            tp[ent[2]] += 1\n",
    "        else:\n",
    "            fp[ent[2]] += 1\n",
    "\n",
    "    for ent in gt_set:\n",
    "        if ent not in pred_set:\n",
    "            fn[ent[2]] += 1\n",
    "\n",
    "    return tp, fp, fn\n",
    "\n",
    "\n",
    "def compute_confusion(gt_mapping: Dict[str, Dict[str, List[Entity]]],\n",
    "                      pred_mapping: Dict[str, List[Entity]]\n",
    "                     ) -> Tuple[Counter, Counter, Counter]:\n",
    "    tp_total = Counter()\n",
    "    fp_total = Counter()\n",
    "    fn_total = Counter()\n",
    "\n",
    "    # Files present in either set\n",
    "    all_files = set(gt_mapping.keys()) | set(pred_mapping.keys())\n",
    "    for file in all_files:\n",
    "        gt_labels = gt_mapping.get(file, {})  # could be empty\n",
    "        pred_entities = pred_mapping.get(file, [])\n",
    "        tp, fp, fn = compute_confusion_single_file(gt_labels, pred_entities)\n",
    "        tp_total.update(tp)\n",
    "        fp_total.update(fp)\n",
    "        fn_total.update(fn)\n",
    "\n",
    "    return tp_total, fp_total, fn_total\n",
    "\n",
    "\n",
    "def precision_recall_f1(tp: int, fp: int, fn: int):\n",
    "    prec = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "    rec = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "    if prec + rec == 0:\n",
    "        f1 = 0.0\n",
    "    else:\n",
    "        f1 = 2 * prec * rec / (prec + rec)\n",
    "    return prec, rec, f1\n",
    "\n",
    "\n",
    "def aggregate_metrics(tp: Counter, fp: Counter, fn: Counter):\n",
    "    per_label = {}\n",
    "    for label in sorted(set(list(tp.keys()) + list(fp.keys()) + list(fn.keys()))):\n",
    "        p, r, f1 = precision_recall_f1(tp[label], fp[label], fn[label])\n",
    "        per_label[label] = {\n",
    "            \"precision\": p,\n",
    "            \"recall\": r,\n",
    "            \"f1\": f1,\n",
    "            \"tp\": tp[label],\n",
    "            \"fp\": fp[label],\n",
    "            \"fn\": fn[label],\n",
    "        }\n",
    "\n",
    "    total_tp = sum(tp.values())\n",
    "    total_fp = sum(fp.values())\n",
    "    total_fn = sum(fn.values())\n",
    "    micro_p, micro_r, micro_f1 = precision_recall_f1(total_tp, total_fp, total_fn)\n",
    "\n",
    "    macro_f1 = sum(v[\"f1\"] for v in per_label.values()) / max(len(per_label), 1)\n",
    "\n",
    "    return {\n",
    "        \"per_label\": per_label,\n",
    "        \"micro\": {\"precision\": micro_p, \"recall\": micro_r, \"f1\": micro_f1},\n",
    "        \"macro_f1\": macro_f1,\n",
    "        \"totals\": {\"tp\": total_tp, \"fp\": total_fp, \"fn\": total_fn},\n",
    "    }\n",
    "\n",
    "\n",
    "def print_summary(overall_metrics: Dict, per_file_metrics: Dict[str, Dict]):\n",
    "    def fmt(x):\n",
    "        return f\"{x:.3f}\"\n",
    "\n",
    "    print(\"\\n=== OVERALL ===\")\n",
    "    print(\"Micro precision:  \", fmt(overall_metrics[\"micro\"][\"precision\"]))\n",
    "    print(\"Micro recall:     \", fmt(overall_metrics[\"micro\"][\"recall\"]))\n",
    "    print(\"Micro F1:         \", fmt(overall_metrics[\"micro\"][\"f1\"]))\n",
    "    print(\"Macro F1:         \", fmt(overall_metrics[\"macro_f1\"]))\n",
    "    print(\"Totals (TP/FP/FN):\", overall_metrics[\"totals\"][\"tp\"],\n",
    "          overall_metrics[\"totals\"][\"fp\"], overall_metrics[\"totals\"][\"fn\"])\n",
    "    print(\"\\nPer-label breakdown:\")\n",
    "    headers = [\"Label\", \"TP\", \"FP\", \"FN\", \"Precision\", \"Recall\", \"F1\"]\n",
    "    print(f\"{headers[0]:20} {headers[1]:>3} {headers[2]:>3} {headers[3]:>3} {headers[4]:>9} {headers[5]:>7} {headers[6]:>6}\")\n",
    "    for label, stats in overall_metrics[\"per_label\"].items():\n",
    "        print(f\"{label:20} {stats['tp']:3} {stats['fp']:3} {stats['fn']:3} \"\n",
    "              f\"{fmt(stats['precision']):>9} {fmt(stats['recall']):>7} {fmt(stats['f1']):>6}\")\n",
    "\n",
    "    print(\"\\n=== PER FILE ===\")\n",
    "    for fname, metrics in sorted(per_file_metrics.items()):\n",
    "        print(f\"\\nFile: {fname}\")\n",
    "        print(\"  Micro P / R / F1:\",\n",
    "              fmt(metrics[\"micro\"][\"precision\"]), \"/\",\n",
    "              fmt(metrics[\"micro\"][\"recall\"]), \"/\",\n",
    "              fmt(metrics[\"micro\"][\"f1\"]))\n",
    "        print(\"  Totals (TP/FP/FN):\", metrics[\"totals\"][\"tp\"],\n",
    "              metrics[\"totals\"][\"fp\"], metrics[\"totals\"][\"fn\"])\n",
    "        # optionally per-label for that file:\n",
    "        for label, stats in metrics[\"per_label\"].items():\n",
    "            print(f\"    {label:15} TP={stats['tp']} FP={stats['fp']} FN={stats['fn']} \"\n",
    "                  f\"P={fmt(stats['precision'])} R={fmt(stats['recall'])} F1={fmt(stats['f1'])}\")\n",
    "\n",
    "\n",
    "def compute_per_file(gt_mapping: Dict[str, Dict[str, List[Entity]]],\n",
    "                     pred_mapping: Dict[str, List[Entity]]\n",
    "                    ) -> Dict[str, Dict]:\n",
    "    per_file = {}\n",
    "    all_files = set(gt_mapping.keys()) | set(pred_mapping.keys())\n",
    "    for file in all_files:\n",
    "        gt_labels = gt_mapping.get(file, {})\n",
    "        pred_entities = pred_mapping.get(file, [])\n",
    "        tp, fp, fn = compute_confusion_single_file(gt_labels, pred_entities)\n",
    "        metrics = aggregate_metrics(tp, fp, fn)\n",
    "        per_file[file] = metrics\n",
    "    return per_file\n",
    "\n",
    "\n",
    "def main(gt_json_path: str, anonymized_json_path: str):\n",
    "    anonymized = load_json(anonymized_json_path)\n",
    "    ground_truth = load_json(gt_json_path)\n",
    "\n",
    "    gt_map = build_gt_mapping(ground_truth)\n",
    "    pred_map = extract_predictions_from_anonymized(anonymized, gt_map)\n",
    "    tp, fp, fn = compute_confusion(gt_map, pred_map)\n",
    "    overall_metrics = aggregate_metrics(tp, fp, fn)\n",
    "    per_file_metrics = compute_per_file(gt_map, pred_map)\n",
    "\n",
    "    print_summary(overall_metrics, per_file_metrics)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # === hardcode your paths here ===\n",
    "    ground_truth_path = \"../../../data/original/ground_truth_split/test_norm.json\"\n",
    "    anonymized_results_path = \"../../../data/testing/gemini_results/anonymized_text_results/combined_results_1.5.json\"\n",
    "    main(ground_truth_path, anonymized_results_path)"
   ],
   "id": "5329f2841063f1c9"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
