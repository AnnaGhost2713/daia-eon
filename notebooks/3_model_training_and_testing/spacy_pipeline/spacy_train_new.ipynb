{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "762a2b8c087c5716",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WinError 206] Der Dateiname oder die Erweiterung ist zu lang: 'daia-eon/notebooks/3_model_training_and_testing/spacy_pipeline/'\n",
      "c:\\Users\\morit\\OneDrive\\Uni\\02_Master\\05_Studium\\02_Semester_II\\Data Analytics in Applications\\VSCode\\daia-eon\\notebooks\\3_model_training_and_testing\\spacy_pipeline\\daia-eon\\notebooks\\3_model_training_and_testing\\spacy_pipeline\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:/Users/morit/OneDrive/Uni/02_Master/05_Studium/02_Semester_II/Data Analytics in Applications/VSCode/daia-eon/notebooks/3_model_training_and_testing/spacy_pipeline/daia-eon/notebooks/3_model_training_and_testing/spacy_pipeline/daia-eon/.git/hooks/: Filename too long\n",
      "Cloning into 'daia-eon'...\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/AnnaGhost2713/daia-eon.git\n",
    "%cd daia-eon/notebooks/3_model_training_and_testing/spacy_pipeline/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-03T08:53:06.301489Z",
     "start_time": "2025-07-03T08:53:05.599538Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lade 112 Trainingsbeispiele.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "[E103] Trying to set conflicting doc.ents: '(130, 136, 'VORNAME')' and '(130, 136, 'FIRMA')'. A token can only be part of one entity, so make sure the entities you're setting don't overlap. To work with overlapping entities, consider using doc.spans instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 58\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m text, annotations \u001b[38;5;129;01min\u001b[39;00m train_data:\n\u001b[1;32m     57\u001b[0m     doc \u001b[38;5;241m=\u001b[39m nlp\u001b[38;5;241m.\u001b[39mmake_doc(text)\n\u001b[0;32m---> 58\u001b[0m     example \u001b[38;5;241m=\u001b[39m \u001b[43mExample\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mannotations\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m     nlp\u001b[38;5;241m.\u001b[39mupdate([example], drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.35\u001b[39m, losses\u001b[38;5;241m=\u001b[39mlosses)\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIteration \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_iter\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Losses: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlosses\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/piranha-env/lib/python3.10/site-packages/spacy/training/example.pyx:130\u001b[0m, in \u001b[0;36mspacy.training.example.Example.from_dict\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/piranha-env/lib/python3.10/site-packages/spacy/training/example.pyx:34\u001b[0m, in \u001b[0;36mspacy.training.example.annotations_to_doc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/piranha-env/lib/python3.10/site-packages/spacy/training/example.pyx:511\u001b[0m, in \u001b[0;36mspacy.training.example._add_entities_to_doc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/piranha-env/lib/python3.10/site-packages/spacy/training/iob_utils.py:114\u001b[0m, in \u001b[0;36moffsets_to_biluo_tags\u001b[0;34m(doc, entities, missing)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m token_index \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(start_char, end_char):\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m token_index \u001b[38;5;129;01min\u001b[39;00m tokens_in_ents\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m--> 114\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    115\u001b[0m             Errors\u001b[38;5;241m.\u001b[39mE103\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    116\u001b[0m                 span1\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    117\u001b[0m                     tokens_in_ents[token_index][\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    118\u001b[0m                     tokens_in_ents[token_index][\u001b[38;5;241m1\u001b[39m],\n\u001b[1;32m    119\u001b[0m                     tokens_in_ents[token_index][\u001b[38;5;241m2\u001b[39m],\n\u001b[1;32m    120\u001b[0m                 ),\n\u001b[1;32m    121\u001b[0m                 span2\u001b[38;5;241m=\u001b[39m(start_char, end_char, label),\n\u001b[1;32m    122\u001b[0m             )\n\u001b[1;32m    123\u001b[0m         )\n\u001b[1;32m    124\u001b[0m     tokens_in_ents[token_index] \u001b[38;5;241m=\u001b[39m (start_char, end_char, label)\n\u001b[1;32m    125\u001b[0m start_token \u001b[38;5;241m=\u001b[39m starts\u001b[38;5;241m.\u001b[39mget(start_char)\n",
      "\u001b[0;31mValueError\u001b[0m: [E103] Trying to set conflicting doc.ents: '(130, 136, 'VORNAME')' and '(130, 136, 'FIRMA')'. A token can only be part of one entity, so make sure the entities you're setting don't overlap. To work with overlapping entities, consider using doc.spans instead."
     ]
    }
   ],
   "source": [
    "# 🧩 Schritt 1: Imports und Setup\n",
    "import spacy\n",
    "from spacy.tokens import DocBin\n",
    "from spacy.training.example import Example\n",
    "import json\n",
    "import random\n",
    "from pathlib import Path\n",
    "from spacy.util import minibatch\n",
    "!python -m spacy download de_core_news_md\n",
    "!pip install spacy-lookups-data\n",
    "\n",
    "\n",
    "\n",
    "# 📁 Schritt 2: Funktion zum Laden von Daten aus JSON\n",
    "def load_data_from_json(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        raw_data = json.load(f)\n",
    "    if isinstance(raw_data, dict):\n",
    "        raw_data = [raw_data]\n",
    "\n",
    "    TRAIN_DATA = []\n",
    "    for entry in raw_data:\n",
    "        text = entry[\"text\"]\n",
    "        entities = [(label[\"start\"], label[\"end\"], label[\"label\"]) for label in entry[\"labels\"]]\n",
    "        TRAIN_DATA.append((text, {\"entities\": entities}))\n",
    "    return TRAIN_DATA\n",
    "\n",
    "# 🔄 Lade Trainings- und Dev-Daten separat\n",
    "train_data = load_data_from_json(\"../../../data/original/granular_dataset_split/train.json\")\n",
    "dev_data = load_data_from_json(\"../../../data/original/granular_dataset_split/validation.json\")\n",
    "print(f\"📥 Trainingsbeispiele: {len(train_data)}, Dev-Beispiele: {len(dev_data)}\")\n",
    "\n",
    "# 🧠 Schritt 3: Lade spaCy-Basismodell\n",
    "base_model = \"de_core_news_md\"\n",
    "nlp = spacy.load(base_model)\n",
    "\n",
    "\n",
    "# Stelle sicher, dass NER-Komponente existiert\n",
    "if \"ner\" not in nlp.pipe_names:\n",
    "    ner = nlp.add_pipe(\"ner\", last=True)\n",
    "else:\n",
    "    ner = nlp.get_pipe(\"ner\")\n",
    "\n",
    "# Registriere alle Labels aus beiden Datensätzen\n",
    "for dataset in (train_data, dev_data):\n",
    "    for _, annotations in dataset:\n",
    "        for start, end, label in annotations[\"entities\"]:\n",
    "            ner.add_label(label)\n",
    "\n",
    "# 🚀 Schritt 4: Modell-Initialisierung mit allen Daten (nur für Labels!)\n",
    "def get_examples():\n",
    "    for text, ann in train_data + dev_data:\n",
    "        yield Example.from_dict(nlp.make_doc(text), ann)\n",
    "\n",
    "optimizer = nlp.resume_training()\n",
    "\n",
    "\n",
    "# 🏋️ Schritt 5: Training (nur auf Trainingsdaten)\n",
    "n_iter = 20\n",
    "for i in range(n_iter):\n",
    "    random.shuffle(train_data)\n",
    "    losses = {}\n",
    "\n",
    "    batches = minibatch(train_data, size=8)\n",
    "    for batch in batches:\n",
    "        examples = [Example.from_dict(nlp.make_doc(text), ann) for text, ann in batch]\n",
    "        nlp.update(examples, drop=0.35, losses=losses)\n",
    "\n",
    "    print(f\"🔁 Iteration {i+1}/{n_iter}, Loss: {losses['ner']:.4f}\")\n",
    "\n",
    "# 💾 Schritt 6: Modell speichern\n",
    "output_dir = Path(\"custom_spacy_model_new\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "nlp.to_disk(output_dir)\n",
    "print(f\"\\n✅ Modell gespeichert unter: {output_dir.resolve()}\")\n",
    "\n",
    "# 🔍 Schritt 7: Modell laden und auf dev_data testen\n",
    "nlp2 = spacy.load(output_dir)\n",
    "\n",
    "print(\"\\n📊 Evaluation auf dev_data:\")\n",
    "for text, _ in random.sample(dev_data, min(5, len(dev_data))):  # max. 5 Beispiele\n",
    "    doc = nlp2(text)\n",
    "    print(f\"\\n> {text}\")\n",
    "    for ent in doc.ents:\n",
    "        print(f\"  - {ent.text} ({ent.label_})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
