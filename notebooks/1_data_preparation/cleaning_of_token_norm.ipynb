{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This notebook is used for the synthetic emails. For all generated emails with its spans in json format, it is checked whether the created emails are in line with the token borders of the spacy model. If this is the case, the emails are immediately added to the final json. Otherwise, it will be checked whether an extension of one char will lead to a correct borders (e.g. because of following point or comma). If this is applicable, the borders will be adapted and the email is also added to the final json. If this extension does not lead to correct boarders, the email will be added to the failed file.",
   "id": "ebc90d3f19a1e423"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T10:18:44.316492Z",
     "start_time": "2025-07-31T10:16:27.437246Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "import spacy\n",
    "from pathlib import Path\n",
    "\n",
    "# Define input and output paths\n",
    "nlp = spacy.load(\"de_core_news_md\")\n",
    "\n",
    "# Pfade\n",
    "input_path = Path(\"../../data/synthetic/synthetic_mails_option_b.json\")\n",
    "cleaned_path = Path(\"../../data/synthetic/synthetic_mails_option_b_cleaned_new.json\")\n",
    "failed_path = Path(\"../../data/synthetic/synthetic_mails_option_b_failed_new.json\")\n",
    "\n",
    "\n",
    "def try_expand_one_char(doc, text, start, end, label_name):\n",
    "    \"\"\"\n",
    "    Attempts to fix failed span alignment by expanding the label by one character\n",
    "    to the left or right, using spaCy's strict char_span alignment.\n",
    "    \"\"\"\n",
    "    if start > 0:\n",
    "        span = doc.char_span(start - 1, end, label=label_name, alignment_mode=\"strict\")\n",
    "        if span:\n",
    "            return span\n",
    "    if end < len(text):\n",
    "        span = doc.char_span(start, end + 1, label=label_name, alignment_mode=\"strict\")\n",
    "        if span:\n",
    "            return span\n",
    "    return None\n",
    "\n",
    "\n",
    "def clean_labels(entry, nlp):\n",
    "    \"\"\"\n",
    "    Attempts to create valid spaCy-compatible entity spans for a given entry.\n",
    "    Returns:\n",
    "      - cleaned_labels: list of fixed and valid span dictionaries\n",
    "      - failures: list of failed original labels with optional alignment suggestions\n",
    "    \"\"\"\n",
    "    text = entry.get(\"text\", \"\")\n",
    "    labels = entry.get(\"labels\", [])\n",
    "    doc = nlp(text)\n",
    "\n",
    "    cleaned_labels = []\n",
    "    failures = []\n",
    "\n",
    "    for label in labels:\n",
    "        start, end, label_name = label[\"start\"], label[\"end\"], label[\"label\"]\n",
    "\n",
    "        # Attempt 1: strict alignment\n",
    "        span = doc.char_span(start, end, label=label_name, alignment_mode=\"strict\")\n",
    "        if span:\n",
    "            cleaned_labels.append({\n",
    "                \"start\": span.start_char,\n",
    "                \"end\": span.end_char,\n",
    "                \"label\": span.label_\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        # Attempt 2: one character left or right\n",
    "        span = try_expand_one_char(doc, text, start, end, label_name)\n",
    "        if span:\n",
    "            cleaned_labels.append({\n",
    "                \"start\": span.start_char,\n",
    "                \"end\": span.end_char,\n",
    "                \"label\": span.label_\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        # Attempt 3: fallback using token-based alignment (suggestion only)\n",
    "        span_fb = doc.char_span(start, end, label=label_name, alignment_mode=\"expand\")\n",
    "        if span_fb:\n",
    "            suggested = {\n",
    "                \"start\": span_fb.start_char,\n",
    "                \"end\": span_fb.end_char,\n",
    "                \"text\": span_fb.text\n",
    "            }\n",
    "        else:\n",
    "            suggested = None\n",
    "\n",
    "        # Record as failed alignmen\n",
    "        failures.append({\n",
    "            \"orig_label\": {\n",
    "                \"start\": start,\n",
    "                \"end\": end,\n",
    "                \"label\": label_name,\n",
    "                \"text\": text[start:end]\n",
    "            },\n",
    "            \"suggestion\": suggested\n",
    "        })\n",
    "\n",
    "    return cleaned_labels, failures\n",
    "\n",
    "\n",
    "def process_file(input_path, cleaned_path, failed_path):\n",
    "    \"\"\"\n",
    "    Loads annotated JSON data, attempts to fix misaligned entity spans,\n",
    "    and separates clean vs. problematic examples into two output files.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Read the input file\n",
    "    with input_path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    cleaned_data = []\n",
    "    failed_data = []\n",
    "\n",
    "    # 2. Process each entry\n",
    "    for entry in data:\n",
    "        cleaned_labels, failures = clean_labels(entry, nlp)\n",
    "\n",
    "        if failures:\n",
    "            # At least one label failed alignment → store full entry in failed_data\n",
    "            failed_data.append({\n",
    "                \"file\": entry.get(\"file\", \"\"),\n",
    "                \"text\": entry.get(\"text\", \"\"),\n",
    "                \"orig_labels\": entry.get(\"labels\", []),\n",
    "                \"failures\": failures\n",
    "            })\n",
    "        else:\n",
    "            # All labels are valid → store cleaned version\n",
    "            cleaned_data.append({\n",
    "                \"file\": entry.get(\"file\", \"\"),\n",
    "                \"text\": entry.get(\"text\", \"\"),\n",
    "                \"labels\": cleaned_labels\n",
    "            })\n",
    "\n",
    "    # 3. Save cleaned and failed outputs\n",
    "    with cleaned_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(cleaned_data, f, ensure_ascii=False, indent=2)\n",
    "    with failed_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(failed_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    # 4. Report processing results\n",
    "    total_emails = len(data)\n",
    "    total_cleaned = len(cleaned_data)\n",
    "    total_failed_mail = len(failed_data)\n",
    "\n",
    "    print(f\"Total emails processed: {total_emails}\")\n",
    "    print(f\"Fully cleaned entries:     {total_cleaned}\")\n",
    "    print(f\"Entries with label issues (failed):      {total_failed_mail}\\n\")\n",
    "\n",
    "    # Print example failures\n",
    "    if total_failed_mail:\n",
    "        print(\"Examples of failed entries:\")\n",
    "        for rec in failed_data[:5]:\n",
    "            print(f\" • File {rec['file']!r}:\")\n",
    "            for f in rec[\"failures\"]:\n",
    "                orig = f[\"orig_label\"]\n",
    "                print(f\"    – Orig: '{orig['text']}' @{orig['start']}-{orig['end']} ({orig['label']})\")\n",
    "                if f[\"suggestion\"]:\n",
    "                    s = f[\"suggestion\"]\n",
    "                    print(f\"      ↳ Suggested: '{s['text']}' @{s['start']}-{s['end']}\")\n",
    "                else:\n",
    "                    print(\"      ↳ No valid suggestion\")\n",
    "            print()\n",
    "\n",
    "    print(f\"Cleaned output saved to: {cleaned_path.resolve()}\")\n",
    "    print(f\"Failed output saved to:   {failed_path.resolve()}\")\n",
    "\n",
    "\n",
    "# Execute cleaning\n",
    "process_file(input_path, cleaned_path, failed_path)"
   ],
   "id": "6d40b0a0c2d1b1c6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total emails processed: 14360\n",
      "Fully cleaned entries:     13506\n",
      "⚠️  Entries with label issues (failed):      854\n",
      "\n",
      "BExamples of failed entries:\n",
      " • File '16':\n",
      "    – Orig: 'Dipl.-Ing' @121-130 (TITEL)\n",
      "      ↳ Suggested: 'ADipl.-Ingesse' @120-134\n",
      "\n",
      " • File '18':\n",
      "    – Orig: 'Aurelia' @340-347 (VORNAME)\n",
      "      ↳ Suggested: 'Aurelia_Löffler' @340-355\n",
      "    – Orig: 'Löffler' @348-355 (NACHNAME)\n",
      "      ↳ Suggested: 'Aurelia_Löffler' @340-355\n",
      "\n",
      " • File '49':\n",
      "    – Orig: 'Eugen' @205-210 (VORNAME)\n",
      "      ↳ Suggested: 'EugenKlemt' @205-215\n",
      "    – Orig: 'Klemt' @210-215 (NACHNAME)\n",
      "      ↳ Suggested: 'EugenKlemt' @205-215\n",
      "\n",
      " • File '76':\n",
      "    – Orig: 'Mirjam' @184-190 (VORNAME)\n",
      "      ↳ Suggested: 'MirjamDowerg' @184-196\n",
      "    – Orig: 'Dowerg' @190-196 (NACHNAME)\n",
      "      ↳ Suggested: 'MirjamDowerg' @184-196\n",
      "\n",
      " • File '79':\n",
      "    – Orig: 'Klapp' @189-194 (NACHNAME)\n",
      "      ↳ Suggested: 'Klapp402' @189-197\n",
      "    – Orig: '402 862 472 874' @194-209 (VERTRAGSNUMMER)\n",
      "      ↳ Suggested: 'Klapp402 862 472 874' @189-209\n",
      "\n",
      "✅ Cleaned output saved to: /Users/timonmartens/Library/CloudStorage/OneDrive-Persönlich/Desktop/Veranstaltungen/Data Analytics in Applications/daia-eon/data/synthetic/synthetic_mails_option_b_cleaned_new_TEST.json\n",
      "✅ Failed output saved to:   /Users/timonmartens/Library/CloudStorage/OneDrive-Persönlich/Desktop/Veranstaltungen/Data Analytics in Applications/daia-eon/data/synthetic/synthetic_mails_option_b_failed_new_TEST.json\n"
     ]
    }
   ],
   "execution_count": 1
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
