{
 "cells": [
  {
   "metadata": {},
   "cell_type": "raw",
   "source": "",
   "id": "40fab22b9e48b9a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Dataset Descriptives Analysis\n",
    "\n",
    "This notebook loads multiple JSON ground-truth label datasets, computes summary statistics (unique labels, label counts, samples per label, labels per sample, missing labels) for each individual dataset and combined, and creates publication-quality plots plus a summary table.\n",
    "\n",
    "**Inputs:** paths to JSON files containing lists of samples; each sample is expected to have a `labels` field which is a list of dictionaries with a `'label'` key.\n",
    "\n",
    "**Outputs:**\n",
    "- PDF plots: overall label distribution, dataset comparisons (counts and percentages), labels-per-sample histogram, label coverage.\n",
    "- Summary CSV/table with combined and individual metrics.\n",
    "\n",
    "**Main flow:**\n",
    "1. `load_all_datasets`: loads and validates datasets.\n",
    "2. `analyze_dataset`: safely extracts labels and computes metrics.\n",
    "3. `create_publication_plots`: generates and saves the visualizations.\n",
    "4. `create_summary_table`: builds the summary DataFrame.\n",
    "5. `analyze_full_dataset`: orchestrates the steps and persists results.\n",
    "\n",
    "**Assumptions / notes:** label presence per sample is deduplicated (i.e., multiple identical labels in one sample count once), malformed label entries are filtered out, and missing labels are reported."
   ],
   "id": "adf4c38fb67b67be"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "from typing import Dict, List, Set, Tuple\n",
    "import numpy as np\n",
    "\n",
    "# Configure matplotlib for publication quality (larger fonts for readability)\n",
    "plt.rcParams.update({\n",
    "    'font.family': 'serif',\n",
    "    'font.serif': ['Times New Roman', 'Times', 'serif'],\n",
    "    'font.size': 12,  # Increased base font size\n",
    "    'axes.labelsize': 14,  # Larger axis labels\n",
    "    'axes.titlesize': 16,  # Larger titles\n",
    "    'xtick.labelsize': 11,  # Larger tick labels\n",
    "    'ytick.labelsize': 11,  # Larger tick labels\n",
    "    'legend.fontsize': 11,  # Larger legend\n",
    "    'figure.titlesize': 18,  # Larger figure title\n",
    "    'lines.linewidth': 1.5,\n",
    "    'axes.linewidth': 1.2,\n",
    "    'grid.linewidth': 0.8,\n",
    "    'xtick.major.width': 1.2,\n",
    "    'ytick.major.width': 1.2,\n",
    "    'axes.edgecolor': 'black',\n",
    "    'text.color': 'black',\n",
    "    'axes.labelcolor': 'black',\n",
    "    'xtick.color': 'black',\n",
    "    'ytick.color': 'black',\n",
    "    'figure.facecolor': 'white',\n",
    "    'axes.facecolor': 'white',\n",
    "    'savefig.facecolor': 'white',\n",
    "    'axes.spines.left': True,\n",
    "    'axes.spines.bottom': True,\n",
    "    'axes.spines.top': True,\n",
    "    'axes.spines.right': True\n",
    "})\n",
    "\n",
    "# Define the expected labels (from your original code)\n",
    "PLACEHOLDERS = {\n",
    "    \"TITEL\"         :  [\"TITEL\"],\n",
    "    \"VORNAME\"       :  [\"VORNAME\"],\n",
    "    \"NACHNAME\"      :  [\"NACHNAME\"],\n",
    "    \"FIRMA\"         :  [\"FIRMA\"],\n",
    "    \"TELEFONNUMMER\" :  [\"TELEFONNUMMER\"],\n",
    "    \"EMAIL\"         :  [\"EMAIL\"],\n",
    "    \"FAX\"           :  [\"FAX\"],\n",
    "    \"STRASSE\"       :  [\"STRASSE\"],\n",
    "    \"HAUSNUMMER\"    :  [\"HAUSNUMMER\"],\n",
    "    \"POSTLEITZAHL\"  :  [\"POSTLEITZAHL\",\"PLZ\",\"ZIP\"],\n",
    "    \"WOHNORT\"       :  [\"WOHNORT\",\"ORT\",\"CITY\"],\n",
    "    \"Z√ÑHLERNUMMER\"  :  [\"Z√ÑHLERNUMMER\",\"METER_ID\"],\n",
    "    \"Z√ÑHLERSTAND\"   :  [\"Z√ÑHLERSTAND\",\"METER_READING\"],\n",
    "    \"VERTRAGSNUMMER\":  [\"VERTRAGSNUMMER\",\"ANGEBOTSNUMMER\", \"KUNDENNUMMER\"],\n",
    "    \"ZAHLUNG\"       :  [\"BETRAG\",\"ZAHLUNG\",\"AMOUNT\"],\n",
    "    \"BANK\"          :  [\"BANK\"],\n",
    "    \"IBAN\"          :  [\"IBAN\"],\n",
    "    \"BIC\"           :  [\"BIC\"],\n",
    "    \"DATUM\"         :  [\"DATUM\",\"DATE\"],\n",
    "    \"GESENDET_MIT\"  :  [\"GESENDET_MIT\"],\n",
    "    \"LINK\"          :  [\"LINK\"]\n",
    "}\n",
    "\n",
    "EXPECTED_LABELS = set(PLACEHOLDERS.keys())\n",
    "\n",
    "def load_all_datasets(file_paths: List[str]) -> Tuple[List[Dict], Dict[str, List[Dict]]]:\n",
    "    \"\"\"Load multiple datasets and combine them.\"\"\"\n",
    "    all_data = []\n",
    "    individual_datasets = {}\n",
    "\n",
    "    for file_path in file_paths:\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "            dataset_name = file_path.split('/')[-1].replace('.json', '')\n",
    "            individual_datasets[dataset_name] = data\n",
    "            all_data.extend(data)\n",
    "            print(f\"‚úì Loaded {len(data)} samples from {file_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error loading {file_path}: {e}\")\n",
    "\n",
    "    return all_data, individual_datasets\n",
    "\n",
    "def analyze_dataset(data: List[Dict]) -> Dict:\n",
    "    \"\"\"Perform comprehensive analysis of the dataset.\"\"\"\n",
    "    if not data:\n",
    "        return {}\n",
    "\n",
    "    num_samples = len(data)\n",
    "    all_labels = []\n",
    "    samples_with_labels = []\n",
    "\n",
    "    for sample in data:\n",
    "        sample_labels = {label['label'] for label in sample.get('labels', [])}\n",
    "        all_labels.extend(sample_labels)\n",
    "        samples_with_labels.append(sample_labels)\n",
    "\n",
    "    label_counts = Counter(all_labels)\n",
    "    unique_labels = set(all_labels)\n",
    "    missing_labels = EXPECTED_LABELS - unique_labels\n",
    "\n",
    "    samples_per_label = {label: sum(1 for s in samples_with_labels if label in s)\n",
    "                        for label in unique_labels}\n",
    "\n",
    "    labels_per_sample = [len(sample_labels) for sample_labels in samples_with_labels]\n",
    "\n",
    "    return {\n",
    "        'num_samples': num_samples,\n",
    "        'unique_labels': unique_labels,\n",
    "        'label_counts': label_counts,\n",
    "        'missing_labels': missing_labels,\n",
    "        'samples_per_label': samples_per_label,\n",
    "        'labels_per_sample': labels_per_sample\n",
    "    }\n",
    "\n",
    "def create_publication_plots(combined_analysis: Dict, individual_analyses: Dict[str, Dict],\n",
    "                           output_dir: str = \"./figures/\"):\n",
    "    \"\"\"Create publication-quality plots for the dataset analysis.\"\"\"\n",
    "\n",
    "    # Create output directory if it doesn't exist\n",
    "    import os\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Black and white styling for professional appearance\n",
    "    patterns = ['', '///', '...', '+++', 'xxx', '|||', '---', '\\\\\\\\\\\\']\n",
    "    grays = ['0.2', '0.4', '0.6', '0.8', '0.3', '0.5', '0.7', '0.9']\n",
    "\n",
    "    # 1. Combined Label Distribution (Horizontal Bar Chart) - More compact\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))  # Wider figure for better readability\n",
    "\n",
    "    # Sort labels by frequency for better readability\n",
    "    sorted_labels = sorted(combined_analysis['samples_per_label'].items(),\n",
    "                          key=lambda x: x[1], reverse=True)\n",
    "    labels, counts = zip(*sorted_labels)\n",
    "\n",
    "    # Create horizontal bar chart with black and white styling\n",
    "    y_pos = np.arange(len(labels))\n",
    "    bars = ax.barh(y_pos, counts, color='black', alpha=0.8, height=0.6,\n",
    "                   edgecolor='black', linewidth=0.8)\n",
    "\n",
    "    # Formatting with better spacing\n",
    "    ax.set_yticks(y_pos)\n",
    "    ax.set_yticklabels(labels, fontweight='normal')\n",
    "    ax.set_xlabel('Number of Samples', fontweight='bold')\n",
    "    ax.set_title('Label Distribution Across Full Dataset', fontweight='bold', pad=20)\n",
    "    ax.grid(axis='x', alpha=0.3, linewidth=0.8)\n",
    "\n",
    "    # Add value labels on bars with better positioning\n",
    "    for i, bar in enumerate(bars):\n",
    "        width = bar.get_width()\n",
    "        ax.text(width + max(counts) * 0.02, bar.get_y() + bar.get_height()/2,\n",
    "                f'{int(width)}', ha='left', va='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "    # Tighter layout\n",
    "    ax.margins(y=0.01)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{output_dir}label_distribution.pdf', bbox_inches='tight', dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "    # 2. Dataset Comparison (Grouped Bar Chart) - Optimized layout\n",
    "    if len(individual_analyses) > 1:\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))  # Wider for better label readability\n",
    "\n",
    "        # Get top 8 most common labels for readability (reduced from 10)\n",
    "        top_labels = [label for label, _ in sorted_labels[:8]]\n",
    "\n",
    "        # Prepare data for grouped bar chart\n",
    "        dataset_names = list(individual_analyses.keys())\n",
    "        x = np.arange(len(top_labels))\n",
    "        width = 0.7 / len(dataset_names)  # Slightly wider bars\n",
    "\n",
    "        for i, (dataset_name, analysis) in enumerate(individual_analyses.items()):\n",
    "            values = [analysis['samples_per_label'].get(label, 0) for label in top_labels]\n",
    "            offset = (i - len(dataset_names)/2 + 0.5) * width\n",
    "\n",
    "            # Use different patterns and fills for different datasets\n",
    "            if i < len(patterns):\n",
    "                hatch = patterns[i]\n",
    "                facecolor = grays[i % len(grays)]\n",
    "            else:\n",
    "                hatch = ''\n",
    "                facecolor = grays[i % len(grays)]\n",
    "\n",
    "            bars = ax.bar(x + offset, values, width,\n",
    "                         label=dataset_name.replace('_', ' ').title(),\n",
    "                         facecolor=facecolor, edgecolor='black', linewidth=1.0,\n",
    "                         hatch=hatch, alpha=0.8)\n",
    "\n",
    "        ax.set_xlabel('Label Type', fontweight='bold')\n",
    "        ax.set_ylabel('Number of Samples', fontweight='bold')\n",
    "        ax.set_title('Label Distribution Comparison Across Datasets', fontweight='bold', pad=20)\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(top_labels, rotation=45, ha='right', fontweight='normal')\n",
    "        ax.legend(loc='upper right')\n",
    "        ax.grid(axis='y', alpha=0.3, linewidth=0.8)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{output_dir}dataset_comparison.pdf', bbox_inches='tight', dpi=300)\n",
    "        plt.show()\n",
    "\n",
    "    # 2b. Dataset Comparison in Percentage (Grouped Bar Chart)\n",
    "    if len(individual_analyses) > 1:\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "        # Use combined label order from the first (count) chart\n",
    "        all_labels = [label for label, _ in sorted_labels]\n",
    "        dataset_names = list(individual_analyses.keys())\n",
    "        x = np.arange(len(all_labels))\n",
    "        width = 0.7 / len(dataset_names)  # bar width\n",
    "\n",
    "        # Use the same patterns and grays as above\n",
    "        # patterns and grays are already defined earlier in this function\n",
    "\n",
    "        for i, (dataset_name, analysis) in enumerate(individual_analyses.items()):\n",
    "            # Raw counts for every label\n",
    "            counts = [analysis['samples_per_label'].get(label, 0) for label in all_labels]\n",
    "            total = sum(counts) if sum(counts) > 0 else 1\n",
    "            perc = [c / total * 100 for c in counts]\n",
    "\n",
    "            offset = (i - len(dataset_names)/2 + 0.5) * width\n",
    "\n",
    "            bars = ax.bar(\n",
    "                x + offset,\n",
    "                perc,\n",
    "                width,\n",
    "                label=dataset_name.replace('_', ' ').title(),\n",
    "                facecolor=grays[i % len(grays)],\n",
    "                edgecolor='black',\n",
    "                linewidth=1.0,\n",
    "                hatch=patterns[i] if i < len(patterns) else '',\n",
    "                alpha=0.8\n",
    "            )\n",
    "\n",
    "        ax.set_xlabel('Label Type', fontweight='bold')\n",
    "        ax.set_ylabel('Percentage of Label Instances (%)', fontweight='bold')\n",
    "        ax.set_title('Label Distribution Across Datasets (Percentage)', fontweight='bold', pad=20)\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(all_labels, rotation=45, ha='right', fontweight='normal')\n",
    "        ax.legend(loc='upper right')\n",
    "        ax.grid(axis='y', alpha=0.3, linewidth=0.8)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{output_dir}dataset_comparison_percentage.pdf', bbox_inches='tight', dpi=300)\n",
    "        plt.show()\n",
    "\n",
    "    # 3. Labels per Sample Distribution - Simplified and larger\n",
    "    fig, ax = plt.subplots(figsize=(7, 5))\n",
    "\n",
    "    labels_per_sample = combined_analysis['labels_per_sample']\n",
    "    bins = range(min(labels_per_sample), max(labels_per_sample) + 2)\n",
    "\n",
    "    n, bins, patches = ax.hist(labels_per_sample, bins=bins, color='0.6',\n",
    "                              alpha=0.8, edgecolor='black', linewidth=1.0)\n",
    "\n",
    "    ax.set_xlabel('Number of Labels per Sample', fontweight='bold')\n",
    "    ax.set_ylabel('Frequency (Number of Samples)', fontweight='bold')\n",
    "    ax.set_title('Distribution of Labels per Sample', fontweight='bold', pad=20)\n",
    "    ax.grid(axis='y', alpha=0.3, linewidth=0.8)\n",
    "\n",
    "    # Add statistics annotation with black and white styling\n",
    "    mean_labels = np.mean(labels_per_sample)\n",
    "    median_labels = np.median(labels_per_sample)\n",
    "    ax.axvline(mean_labels, color='black', linestyle='--', linewidth=2.0,\n",
    "               label=f'Mean: {mean_labels:.1f}')\n",
    "    ax.axvline(median_labels, color='black', linestyle=':', linewidth=2.0,\n",
    "               label=f'Median: {median_labels:.1f}')\n",
    "    ax.legend(loc='upper right')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{output_dir}labels_per_sample.pdf', bbox_inches='tight', dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "    # 4. Label Coverage Analysis - Optimized for readability\n",
    "    fig, ax = plt.subplots(figsize=(8, 7))  # Taller for better label spacing\n",
    "\n",
    "    # Calculate coverage percentages\n",
    "    coverage_data = []\n",
    "    for label in EXPECTED_LABELS:\n",
    "        if label in combined_analysis['unique_labels']:\n",
    "            coverage = (combined_analysis['samples_per_label'][label] /\n",
    "                       combined_analysis['num_samples']) * 100\n",
    "            coverage_data.append((label, coverage))\n",
    "        else:\n",
    "            coverage_data.append((label, 0))\n",
    "\n",
    "    # Sort by coverage\n",
    "    coverage_data.sort(key=lambda x: x[1], reverse=True)\n",
    "    labels_cov, coverages = zip(*coverage_data)\n",
    "\n",
    "    # Create grayscale styling based on coverage\n",
    "    colors_map = []\n",
    "    hatches = []\n",
    "    for cov in coverages:\n",
    "        if cov == 0:\n",
    "            colors_map.append('white')  # White for missing\n",
    "            hatches.append('///')  # Diagonal lines for missing\n",
    "        elif cov < 25:\n",
    "            colors_map.append('0.8')  # Light gray for very low coverage\n",
    "            hatches.append('')\n",
    "        elif cov < 50:\n",
    "            colors_map.append('0.6')  # Medium gray for low coverage\n",
    "            hatches.append('')\n",
    "        elif cov < 75:\n",
    "            colors_map.append('0.4')  # Darker gray for medium coverage\n",
    "            hatches.append('')\n",
    "        else:\n",
    "            colors_map.append('0.2')  # Dark gray for high coverage\n",
    "            hatches.append('')\n",
    "\n",
    "    y_pos = np.arange(len(labels_cov))\n",
    "    bars = ax.barh(y_pos, coverages, color=colors_map, alpha=1.0, height=0.6,\n",
    "                   edgecolor='black', linewidth=1.0, hatch=hatches)\n",
    "\n",
    "    ax.set_yticks(y_pos)\n",
    "    ax.set_yticklabels(labels_cov, fontweight='normal')\n",
    "    ax.set_xlabel('Coverage Percentage (%)', fontweight='bold')\n",
    "    ax.set_title('Label Coverage Across Full Dataset', fontweight='bold', pad=20)\n",
    "    ax.grid(axis='x', alpha=0.3, linewidth=0.8)\n",
    "    ax.set_xlim(0, 100)\n",
    "\n",
    "    # Add percentage labels with better formatting\n",
    "    for i, bar in enumerate(bars):\n",
    "        width = bar.get_width()\n",
    "        if width > 0:\n",
    "            ax.text(width + 2, bar.get_y() + bar.get_height()/2,\n",
    "                    f'{width:.1f}%', ha='left', va='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "    # Add margins for better spacing\n",
    "    ax.margins(y=0.01)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{output_dir}label_coverage.pdf', bbox_inches='tight', dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"\\n‚úì All plots saved to {output_dir}\")\n",
    "\n",
    "def create_summary_table(combined_analysis: Dict, individual_analyses: Dict[str, Dict]) -> pd.DataFrame:\n",
    "    \"\"\"Create a comprehensive summary table.\"\"\"\n",
    "\n",
    "    summary_data = []\n",
    "\n",
    "    # Add combined dataset row\n",
    "    summary_data.append({\n",
    "        'Dataset': 'Combined',\n",
    "        'Samples': combined_analysis['num_samples'],\n",
    "        'Unique Labels': len(combined_analysis['unique_labels']),\n",
    "        'Total Instances': sum(combined_analysis['label_counts'].values()),\n",
    "        'Avg Labels/Sample': np.mean(combined_analysis['labels_per_sample']),\n",
    "        'Missing Labels': len(combined_analysis['missing_labels'])\n",
    "    })\n",
    "\n",
    "    # Add individual dataset rows\n",
    "    for name, analysis in individual_analyses.items():\n",
    "        summary_data.append({\n",
    "            'Dataset': name.replace('_', ' ').title(),\n",
    "            'Samples': analysis['num_samples'],\n",
    "            'Unique Labels': len(analysis['unique_labels']),\n",
    "            'Total Instances': sum(analysis['label_counts'].values()),\n",
    "            'Avg Labels/Sample': np.mean(analysis['labels_per_sample']),\n",
    "            'Missing Labels': len(analysis['missing_labels'])\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(summary_data)\n",
    "    df['Avg Labels/Sample'] = df['Avg Labels/Sample'].round(2)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Main analysis function\n",
    "def analyze_full_dataset(file_paths: List[str], output_dir: str = \"./figures/\"):\n",
    "    \"\"\"Complete analysis workflow for multiple dataset files.\"\"\"\n",
    "\n",
    "    print(\"üîç Loading and analyzing datasets...\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    # Load all datasets\n",
    "    combined_data, individual_datasets = load_all_datasets(file_paths)\n",
    "\n",
    "    if not combined_data:\n",
    "        print(\"‚ùå No data loaded. Please check file paths.\")\n",
    "        return\n",
    "\n",
    "    # Analyze combined dataset\n",
    "    combined_analysis = analyze_dataset(combined_data)\n",
    "\n",
    "    # Analyze individual datasets\n",
    "    individual_analyses = {}\n",
    "    for name, data in individual_datasets.items():\n",
    "        individual_analyses[name] = analyze_dataset(data)\n",
    "\n",
    "    # Create publication-quality plots\n",
    "    create_publication_plots(combined_analysis, individual_analyses, output_dir)\n",
    "\n",
    "    # Create summary table\n",
    "    summary_df = create_summary_table(combined_analysis, individual_analyses)\n",
    "\n",
    "    print(\"\\nüìä DATASET SUMMARY\")\n",
    "    print(\"=\" * 60)\n",
    "    print(summary_df.to_string(index=False))\n",
    "\n",
    "    # Save summary table\n",
    "    summary_df.to_csv(f'{output_dir}dataset_summary.csv', index=False)\n",
    "    print(f\"\\n‚úì Summary table saved to {output_dir}dataset_summary.csv\")\n",
    "\n",
    "    return combined_analysis, individual_analyses, summary_df\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Define your dataset files\n",
    "    dataset_files = [\n",
    "        \"../../../data/original/ground_truth_split/train_norm.json\",\n",
    "        \"../../../data/original/ground_truth_split/validation_norm.json\",\n",
    "        \"../../../data/original/ground_truth_split/test_norm.json\"\n",
    "    ]\n",
    "\n",
    "    # Run the complete analysis\n",
    "    combined_analysis, individual_analyses, summary_df = analyze_full_dataset(\n",
    "        dataset_files,\n",
    "        output_dir=\"./publication_figures/\"\n",
    "    )\n"
   ],
   "id": "30502bf1319956c3"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
