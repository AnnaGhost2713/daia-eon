{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-13T13:48:35.418115Z",
     "start_time": "2025-07-13T13:48:35.385960Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Loaded 160 samples\n",
      "\n",
      "Splitting data...\n",
      "Total samples: 160\n",
      "Total unique labels: 21\n",
      "Label distribution: {'ZAHLUNG': 12, 'LINK': 7, 'GESENDET_MIT': 25, 'WOHNORT': 58, 'NACHNAME': 153, 'VORNAME': 145, 'HAUSNUMMER': 56, 'STRASSE': 59, 'VERTRAGSNUMMER': 61, 'POSTLEITZAHL': 55, 'DATUM': 53, 'IBAN': 6, 'TITEL': 13, 'TELEFONNUMMER': 32, 'ZÄHLERNUMMER': 35, 'ZÄHLERSTAND': 10, 'FAX': 4, 'FIRMA': 18, 'BIC': 1, 'BANK': 3, 'EMAIL': 13}\n",
      "Rare labels (<=5 samples): {'FAX', 'BIC', 'BANK'}\n",
      "Distributed 4 samples for rare label 'FAX'\n",
      "Distributed 3 samples for rare label 'BANK'\n",
      "Added 0 BIC samples to training set\n",
      "\n",
      "Label coverage:\n",
      "Test set: 20 labels\n",
      "Validation set: 20 labels\n",
      "Training set: 21 labels\n",
      "\n",
      "Saving datasets to ../../../data/original/granular_dataset_split_norm...\n",
      "\n",
      "==================================================\n",
      "DATASET SPLIT STATISTICS\n",
      "==================================================\n",
      "\n",
      "Training Set:\n",
      "  Samples: 95\n",
      "  Unique labels: 21\n",
      "  Total label instances: 510\n",
      "  Label distribution: {'NACHNAME': 93, 'VORNAME': 88, 'STRASSE': 42, 'WOHNORT': 40, 'HAUSNUMMER': 39, 'POSTLEITZAHL': 38, 'VERTRAGSNUMMER': 35, 'DATUM': 28, 'ZÄHLERNUMMER': 27, 'TELEFONNUMMER': 18, 'GESENDET_MIT': 15, 'FIRMA': 11, 'EMAIL': 9, 'ZAHLUNG': 7, 'TITEL': 7, 'LINK': 4, 'ZÄHLERSTAND': 4, 'IBAN': 2, 'BIC': 1, 'BANK': 1, 'FAX': 1}\n",
      "\n",
      "Validation Set:\n",
      "  Samples: 25\n",
      "  Unique labels: 20\n",
      "  Total label instances: 110\n",
      "  Label distribution: {'NACHNAME': 23, 'VORNAME': 20, 'DATUM': 11, 'VERTRAGSNUMMER': 10, 'GESENDET_MIT': 5, 'WOHNORT': 5, 'HAUSNUMMER': 5, 'STRASSE': 5, 'POSTLEITZAHL': 5, 'FIRMA': 4, 'TELEFONNUMMER': 3, 'TITEL': 3, 'ZÄHLERNUMMER': 2, 'ZÄHLERSTAND': 2, 'ZAHLUNG': 2, 'IBAN': 1, 'BANK': 1, 'LINK': 1, 'FAX': 1, 'EMAIL': 1}\n",
      "\n",
      "Test Set:\n",
      "  Samples: 40\n",
      "  Unique labels: 20\n",
      "  Total label instances: 199\n",
      "  Label distribution: {'NACHNAME': 37, 'VORNAME': 37, 'VERTRAGSNUMMER': 16, 'DATUM': 14, 'WOHNORT': 13, 'HAUSNUMMER': 12, 'STRASSE': 12, 'POSTLEITZAHL': 12, 'TELEFONNUMMER': 11, 'ZÄHLERNUMMER': 6, 'GESENDET_MIT': 5, 'ZÄHLERSTAND': 4, 'ZAHLUNG': 3, 'IBAN': 3, 'TITEL': 3, 'FIRMA': 3, 'EMAIL': 3, 'LINK': 2, 'FAX': 2, 'BANK': 1}\n",
      "\n",
      "✓ Dataset split completed successfully!\n",
      "Files saved in: ../../../data/original/granular_dataset_split_norm\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Script to split JSON dataset into train/validation/test sets\n",
    "ensuring all labels appear in test set with good representation.\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "from collections import defaultdict, Counter\n",
    "from typing import List, Dict, Tuple, Set\n",
    "\n",
    "def load_json_data(filepath: str) -> List[Dict]:\n",
    "    \"\"\"Load JSON data from file.\"\"\"\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def save_json_data(data: List[Dict], filepath: str) -> None:\n",
    "    \"\"\"Save data to JSON file.\"\"\"\n",
    "    os.makedirs(os.path.dirname(filepath), exist_ok=True)\n",
    "    with open(filepath, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "def get_labels_from_sample(sample: Dict) -> Set[str]:\n",
    "    \"\"\"Extract unique labels from a sample.\"\"\"\n",
    "    return {label['label'] for label in sample.get('labels', [])}\n",
    "\n",
    "def analyze_label_distribution(data: List[Dict]) -> Dict[str, int]:\n",
    "    \"\"\"Analyze label frequency across the dataset.\"\"\"\n",
    "    label_counts = Counter()\n",
    "    for sample in data:\n",
    "        labels = get_labels_from_sample(sample)\n",
    "        label_counts.update(labels)\n",
    "    return dict(label_counts)\n",
    "\n",
    "def get_samples_with_labels(data: List[Dict], target_labels: Set[str]) -> List[int]:\n",
    "    \"\"\"Get indices of samples that contain any of the target labels.\"\"\"\n",
    "    indices = []\n",
    "    for i, sample in enumerate(data):\n",
    "        sample_labels = get_labels_from_sample(sample)\n",
    "        if sample_labels.intersection(target_labels):\n",
    "            indices.append(i)\n",
    "    return indices\n",
    "\n",
    "def stratified_split(data: List[Dict], test_size: int = 40, val_size: int = 25) -> Tuple[List[Dict], List[Dict], List[Dict]]:\n",
    "    \"\"\"\n",
    "    Split data ensuring all labels appear in all sets with good representation.\n",
    "\n",
    "    Strategy:\n",
    "    1. Identify all unique labels and their frequencies\n",
    "    2. Ensure each label appears in all sets (except BIC stays in training)\n",
    "    3. Use stratified approach to maintain proportional distribution\n",
    "    4. Prioritize rare labels to appear in multiple sets\n",
    "    \"\"\"\n",
    "    print(f\"Total samples: {len(data)}\")\n",
    "\n",
    "    # Analyze label distribution\n",
    "    label_counts = analyze_label_distribution(data)\n",
    "    all_labels = set(label_counts.keys())\n",
    "    print(f\"Total unique labels: {len(all_labels)}\")\n",
    "    print(f\"Label distribution: {label_counts}\")\n",
    "\n",
    "    # BIC should only be in training set\n",
    "    bic_only_labels = {'BIC'}\n",
    "    labels_for_all_sets = all_labels - bic_only_labels\n",
    "\n",
    "    # Find rare labels (appearing in <=5 samples)\n",
    "    rare_labels = {label for label, count in label_counts.items() if count <= 5}\n",
    "    print(f\"Rare labels (<=5 samples): {rare_labels}\")\n",
    "\n",
    "    # Create sample-to-labels mapping\n",
    "    sample_labels = [get_labels_from_sample(sample) for sample in data]\n",
    "\n",
    "    # Initialize sets\n",
    "    test_indices = set()\n",
    "    val_indices = set()\n",
    "    train_indices = set()\n",
    "\n",
    "    # Step 1: Distribute samples with rare labels across all sets\n",
    "    for label in rare_labels:\n",
    "        if label in bic_only_labels:\n",
    "            continue  # BIC will be handled separately\n",
    "\n",
    "        samples_with_label = [i for i, labels in enumerate(sample_labels)\n",
    "                             if label in labels]\n",
    "\n",
    "        if len(samples_with_label) == 0:\n",
    "            continue\n",
    "\n",
    "        # Ensure this rare label appears in all sets\n",
    "        random.shuffle(samples_with_label)\n",
    "\n",
    "        # Distribute: at least 1 in each set, rest proportionally\n",
    "        if len(samples_with_label) >= 3:\n",
    "            # At least 1 in each set\n",
    "            test_indices.add(samples_with_label[0])\n",
    "            val_indices.add(samples_with_label[1])\n",
    "            train_indices.add(samples_with_label[2])\n",
    "\n",
    "            # Distribute remaining proportionally\n",
    "            remaining = samples_with_label[3:]\n",
    "            if remaining:\n",
    "                # Rough proportion: 40% test, 25% val, 35% train\n",
    "                n_test = max(1, int(len(remaining) * 0.4))\n",
    "                n_val = max(1, int(len(remaining) * 0.25))\n",
    "\n",
    "                test_indices.update(remaining[:n_test])\n",
    "                val_indices.update(remaining[n_test:n_test+n_val])\n",
    "                train_indices.update(remaining[n_test+n_val:])\n",
    "        else:\n",
    "            # Very rare labels: put in training set primarily\n",
    "            train_indices.update(samples_with_label)\n",
    "\n",
    "        print(f\"Distributed {len(samples_with_label)} samples for rare label '{label}'\")\n",
    "\n",
    "    # Step 2: Handle BIC samples (only in training)\n",
    "    bic_samples = [i for i, labels in enumerate(sample_labels)\n",
    "                   if 'BIC' in labels and i not in train_indices]\n",
    "    train_indices.update(bic_samples)\n",
    "    print(f\"Added {len(bic_samples)} BIC samples to training set\")\n",
    "\n",
    "    # Step 3: Fill remaining slots with common labels\n",
    "    used_indices = test_indices | val_indices | train_indices\n",
    "    remaining_samples = [i for i in range(len(data)) if i not in used_indices]\n",
    "\n",
    "    # Shuffle for randomness\n",
    "    random.shuffle(remaining_samples)\n",
    "\n",
    "    # Calculate how many more samples we need for each set\n",
    "    test_needed = max(0, test_size - len(test_indices))\n",
    "    val_needed = max(0, val_size - len(val_indices))\n",
    "\n",
    "    # Fill test set\n",
    "    if test_needed > 0:\n",
    "        test_additional = remaining_samples[:test_needed]\n",
    "        test_indices.update(test_additional)\n",
    "        remaining_samples = remaining_samples[test_needed:]\n",
    "\n",
    "    # Fill validation set\n",
    "    if val_needed > 0:\n",
    "        val_additional = remaining_samples[:val_needed]\n",
    "        val_indices.update(val_additional)\n",
    "        remaining_samples = remaining_samples[val_needed:]\n",
    "\n",
    "    # Rest goes to training\n",
    "    train_indices.update(remaining_samples)\n",
    "\n",
    "    # Create final datasets\n",
    "    test_data = [data[i] for i in test_indices]\n",
    "    val_data = [data[i] for i in val_indices]\n",
    "    train_data = [data[i] for i in train_indices]\n",
    "\n",
    "    # Verify label distribution\n",
    "    test_labels = set()\n",
    "    val_labels = set()\n",
    "    train_labels = set()\n",
    "\n",
    "    for sample in test_data:\n",
    "        test_labels.update(get_labels_from_sample(sample))\n",
    "    for sample in val_data:\n",
    "        val_labels.update(get_labels_from_sample(sample))\n",
    "    for sample in train_data:\n",
    "        train_labels.update(get_labels_from_sample(sample))\n",
    "\n",
    "    print(f\"\\nLabel coverage:\")\n",
    "    print(f\"Test set: {len(test_labels)} labels\")\n",
    "    print(f\"Validation set: {len(val_labels)} labels\")\n",
    "    print(f\"Training set: {len(train_labels)} labels\")\n",
    "\n",
    "    # Check for labels missing from train/val (except BIC)\n",
    "    missing_from_test = labels_for_all_sets - test_labels\n",
    "    missing_from_val = labels_for_all_sets - val_labels\n",
    "    missing_from_train = (all_labels - train_labels)\n",
    "\n",
    "    if missing_from_test:\n",
    "        print(f\"Warning: Labels missing from test set: {missing_from_test}\")\n",
    "    if missing_from_val:\n",
    "        print(f\"Warning: Labels missing from validation set: {missing_from_val}\")\n",
    "    if missing_from_train:\n",
    "        print(f\"Warning: Labels missing from training set: {missing_from_train}\")\n",
    "\n",
    "    return train_data, val_data, test_data\n",
    "\n",
    "def print_split_statistics(train_data: List[Dict], val_data: List[Dict], test_data: List[Dict]) -> None:\n",
    "    \"\"\"Print statistics about the data split.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"DATASET SPLIT STATISTICS\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    datasets = [\n",
    "        (\"Training\", train_data),\n",
    "        (\"Validation\", val_data),\n",
    "        (\"Test\", test_data)\n",
    "    ]\n",
    "\n",
    "    for name, data in datasets:\n",
    "        print(f\"\\n{name} Set:\")\n",
    "        print(f\"  Samples: {len(data)}\")\n",
    "\n",
    "        # Count labels\n",
    "        label_counts = analyze_label_distribution(data)\n",
    "        print(f\"  Unique labels: {len(label_counts)}\")\n",
    "        print(f\"  Total label instances: {sum(label_counts.values())}\")\n",
    "\n",
    "        # Show label distribution\n",
    "        sorted_labels = sorted(label_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "        print(f\"  Label distribution: {dict(sorted_labels)}\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to execute the dataset split.\"\"\"\n",
    "    # Set random seed for reproducibility\n",
    "    random.seed(42)\n",
    "\n",
    "    # Define paths\n",
    "    input_path = \"../../../data/original/golden_dataset_with_spans_norm.json\"\n",
    "    output_dir = \"../../../data/original/granular_dataset_split_norm\"\n",
    "\n",
    "    # Load data\n",
    "    print(\"Loading data...\")\n",
    "    try:\n",
    "        data = load_json_data(input_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Could not find input file at {input_path}\")\n",
    "        return\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Error: Invalid JSON in file {input_path}\")\n",
    "        return\n",
    "\n",
    "    print(f\"Loaded {len(data)} samples\")\n",
    "\n",
    "    # Split data\n",
    "    print(\"\\nSplitting data...\")\n",
    "    train_data, val_data, test_data = stratified_split(data, test_size=40, val_size=25)\n",
    "\n",
    "    # Save split datasets\n",
    "    print(f\"\\nSaving datasets to {output_dir}...\")\n",
    "    save_json_data(train_data, os.path.join(output_dir, \"train_norm.json\"))\n",
    "    save_json_data(val_data, os.path.join(output_dir, \"validation_norm.json\"))\n",
    "    save_json_data(test_data, os.path.join(output_dir, \"test_norm.json\"))\n",
    "\n",
    "    # Print statistics\n",
    "    print_split_statistics(train_data, val_data, test_data)\n",
    "\n",
    "    print(f\"\\n✓ Dataset split completed successfully!\")\n",
    "    print(f\"Files saved in: {output_dir}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
