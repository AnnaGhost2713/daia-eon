{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c6ce6b0-1c91-4529-b76c-baf3ffc2350c",
   "metadata": {},
   "source": [
    "# 02_piiranha_anonymization.ipynb\n",
    "\n",
    "This notebook uses the Piiranha model to detect PII in customer emails and replaces detected PII with placeholders.\n",
    "\n",
    "Model: [iiiorg/piiranha-v1-detect-personal-information](https://huggingface.co/iiiorg/piiranha-v1-detect-personal-information)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0af09286-3a44-418a-ade9-f8fdd1a5e85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing required packages \n",
    "!pip install --quiet --upgrade torch transformers\n",
    "# (only needs to be done once for installing) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10a1561c-270c-4e9e-940d-4dda563c0193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.2\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "335289a2-0d14-4adf-8337-d316ab992771",
   "metadata": {},
   "outputs": [],
   "source": [
    "#(Optional)\n",
    "import torch\n",
    "\n",
    "# Patch missing torch.get_default_device if needed (for PyTorch < 2.3.0)\n",
    "if not hasattr(torch, \"get_default_device\"):\n",
    "    torch.get_default_device = lambda: torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eff683e5-a5de-48dd-890c-a22a9a98afc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DebertaV2ForTokenClassification(\n",
       "  (deberta): DebertaV2Model(\n",
       "    (embeddings): DebertaV2Embeddings(\n",
       "      (word_embeddings): Embedding(251000, 768, padding_idx=0)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): DebertaV2Encoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x DebertaV2Layer(\n",
       "          (attention): DebertaV2Attention(\n",
       "            (self): DisentangledSelfAttention(\n",
       "              (query_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (pos_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): DebertaV2SelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): DebertaV2Intermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): DebertaV2Output(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (rel_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=18, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading the piiranha model\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "\n",
    "model_name = \"iiiorg/piiranha-v1-detect-personal-information\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ee71a1c3-7c31-4211-85d0-127164e6abd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping Piiranha's Labels to unified placeholder tags \n",
    "# (probably needs to be refined again -> WIP here)\n",
    "from collections import defaultdict\n",
    "\n",
    "piiranha_to_placeholder = {\n",
    "    \"NAME\": \"<<NAME>>\",\n",
    "    \"EMAIL\": \"<<EMAIL>>\",\n",
    "    \"PHONE\": \"<<PHONE>>\",\n",
    "    \"ADDRESS\": \"<<ADDRESS>>\",\n",
    "    \"ZIP\": \"<<ADDRESS>>\",\n",
    "    \"CITY\": \"<<ADDRESS>>\",\n",
    "    \"IBAN\": \"<<IBAN>>\",\n",
    "    \"BIC\": \"<<IBAN>>\",\n",
    "    \"CONTRACT\": \"<<CONTRACT>>\",\n",
    "    \"DATE\": \"<<DATE>>\",\n",
    "    \"MONEY\": \"<<MONEY>>\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5abc5039-77af-4b48-b8ef-420268ea7e7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "# Using Hugging Face's pipeline for token classification\n",
    "from transformers import pipeline\n",
    "\n",
    "nlp = pipeline(\n",
    "    \"token-classification\",\n",
    "    model=\"iiiorg/piiranha-v1-detect-personal-information\",\n",
    "    aggregation_strategy=\"simple\",  # Group contiguous tokens into entities\n",
    "    device=0 if torch.cuda.is_available() else -1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4b268f0d-7e3d-40e9-8cb4-d65b725f859e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "def anonymize_combined(text):\n",
    "    # --- Step 1: Run Piiranha via HF pipeline ---\n",
    "    entities = nlp(text)\n",
    "    entities = sorted(entities, key=lambda x: x[\"start\"], reverse=True)\n",
    "    redacted_text = list(text)\n",
    "\n",
    "    for ent in entities:\n",
    "        start, end = ent[\"start\"], ent[\"end\"]\n",
    "        redacted_text[start:end] = list(\"<<PII>>\")  # or use \"<<ADDRESS>>\" if confident\n",
    "\n",
    "    partially_anonymized = ''.join(redacted_text)\n",
    "\n",
    "    # --- Step 2: Apply custom regex rules ---\n",
    "    custom_anonymized = partially_anonymized\n",
    "\n",
    "    # IBAN (German format)\n",
    "    custom_anonymized = re.sub(r'\\bDE\\d{20}\\b', '<<IBAN>>', custom_anonymized)\n",
    "\n",
    "    # Euro payments like 130,50€ or 100.00 €\n",
    "    custom_anonymized = re.sub(r'\\d{1,4}[.,]\\d{2} ?€', '<<MONEY>>', custom_anonymized)\n",
    "\n",
    "    # Dates like 23. Januar or 5 März\n",
    "    custom_anonymized = re.sub(\n",
    "        r'\\b\\d{1,2}\\.?\\s?(Januar|Februar|März|April|Mai|Juni|Juli|August|September|Oktober|November|Dezember)\\b',\n",
    "        '<<DATE>>', custom_anonymized)\n",
    "\n",
    "    # Phone numbers like 0176-12345678 or 089 12345678\n",
    "    custom_anonymized = re.sub(r'\\b0\\d{2,4}[- ]?\\d{5,}\\b', '<<PHONE>>', custom_anonymized)\n",
    "\n",
    "    # Contract numbers (simplified to 6+ digit numbers)\n",
    "    custom_anonymized = re.sub(r'\\b\\d{6,}\\b', '<<CONTRACT>>', custom_anonymized)\n",
    "\n",
    "    return custom_anonymized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b67b2531-816d-4e10-bd8a-0b8a7fed90e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hallo E.ON,\n",
      "\n",
      "mein Name ist Max Mustermann. Ich wohne in der<<PII>><<PII>><<PII>>.\n",
      "Meine Telefonnummer ist <<PHONE>>. Meine Vertragsnummer lautet <<CONTRACT>>.\n",
      "Am <<DATE>> habe ich <<MONEY>> überwiesen. Meine IBAN ist <<IBAN>>.\n",
      "\n",
      "Viele Grüße,\n",
      "Max\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Trying it on a sample mail \n",
    "test_email = \"\"\"\n",
    "Hallo E.ON,\n",
    "\n",
    "mein Name ist Max Mustermann. Ich wohne in der Beispielstraße 8, 80333 München.\n",
    "Meine Telefonnummer ist 0176-12345678. Meine Vertragsnummer lautet 12345678.\n",
    "Am 23. Januar habe ich 130,50€ überwiesen. Meine IBAN ist DE89370400440532013000.\n",
    "\n",
    "Viele Grüße,\n",
    "Max\n",
    "\"\"\"\n",
    "\n",
    "print(anonymize_combined(test_email))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1799e7-7cdd-4f00-91da-8e05b976974a",
   "metadata": {},
   "source": [
    "### NEXT STEPS TO BE DONE ###\n",
    "# Problem: \n",
    "-> piiranha is a nice base option but doesn't work well on German data, neither does it detect all our tags\n",
    "\n",
    "# Solution: \n",
    "-> training a custom Named Entity Recognition (NER) model that\n",
    "    - works in German\n",
    "    - predicts defined tags\n",
    "    - can be reused for future E.ON anonymization projects\n",
    "\n",
    "# Options: \n",
    "1) spaCy\n",
    "+ quick start, ideal for structured NER\n",
    "- slightly less accurate than transformers\n",
    "  \n",
    "2) transformers (e.g. BERT)\n",
    "+ state-of-the-art accuracy, flexible\n",
    "- more complex, needs GPU\n",
    "\n",
    "\n",
    "# Next steps acc. to ChatGPT: \n",
    "# Step 1: Convert your labeled emails to training format\n",
    "Use BIO tagging (Begin-Inside-Outside)\n",
    "\n",
    "Convert them to one of the formats supported by your training framework (e.g., spaCy, CONLL, or CSV)\n",
    "\n",
    "Example format (CONLL-style):\n",
    "\n",
    "mathematica\n",
    "Kopieren\n",
    "Bearbeiten\n",
    "Hallo   O\n",
    "mein    O\n",
    "Name    O\n",
    "ist     O\n",
    "Max     B-VORNAME\n",
    "Mustermann  B-NACHNAME\n",
    ".       O\n",
    "\n",
    "# Step 2: Choose a framework to train your model\n",
    "If you want a fast start, I recommend:\n",
    "🔥 spaCy 3.x, using the official project template for NER\n",
    "\n",
    "If you want transformer-level accuracy:\n",
    "🚀 Hugging Face transformers using token-classification with bert-base-german-cased or deepset/gbert-base\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "da547c63-ca33-411d-b0bf-d0805d01b0d3",
   "metadata": {},
   "source": [
    "| Step | Task                                                                  | Estimated Time          |\n",
    "| ---- | --------------------------------------------------------------------- | ----------------------- |\n",
    "| 1️⃣  | **Convert your labeled data** (e.g., 50 emails) to BIO or JSON format | \\~1–2 hours (with help) |\n",
    "| 2️⃣  | **Prepare dataset** with Hugging Face Datasets (`train`, `val`)       | \\~30–60 minutes         |\n",
    "| 3️⃣  | **Setup training script** using `transformers` Trainer                | \\~1 hour                |\n",
    "| 4️⃣  | **Train the model** on CPU (\\~slow) or Colab GPU                      | \\~20–60 min (with GPU)  |\n",
    "| 5️⃣  | **Evaluate vs. gold standard**                                        | \\~30 minutes            |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4befd814-0bc4-49ef-aa50-e3489829073f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (clean-venv)",
   "language": "python",
   "name": "clean-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
