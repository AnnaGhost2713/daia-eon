{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This notebook is used for the synthetic emails. For all generated emails with its spans in json format, it is checked whether the created emails are in line with the token borders of the spacy model. If this is the case, the emails are immediately added to the final json. Otherwise, it will be checked whether an extension of one char will lead to a correct borders (e.g. because of following point or comma). If this is applicable, the borders will be adapted and the email is also added to the final json. If this extension does not lead to correct boarders, the email will be added to the failed file.",
   "id": "ebc90d3f19a1e423"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-17T18:48:48.351934Z",
     "start_time": "2025-07-17T18:46:32.477250Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "import spacy\n",
    "from pathlib import Path\n",
    "\n",
    "# Lade SpaCy-Modell\n",
    "nlp = spacy.load(\"de_core_news_md\")\n",
    "\n",
    "# Pfade\n",
    "input_path   = Path(\"../../data/synthetic/synthetic_mails_option_b.json\")\n",
    "cleaned_path = Path(\"../../data/synthetic/synthetic_mails_option_b_cleaned_new.json\")\n",
    "failed_path  = Path(\"../../data/synthetic/synthetic_mails_option_b_failed_new.json\")\n",
    "\n",
    "def try_expand_one_char(doc, text, start, end, label_name):\n",
    "    # Ein Zeichen links\n",
    "    if start > 0:\n",
    "        span = doc.char_span(start-1, end, label=label_name, alignment_mode=\"strict\")\n",
    "        if span:\n",
    "            return span\n",
    "    # Ein Zeichen rechts\n",
    "    if end < len(text):\n",
    "        span = doc.char_span(start, end+1, label=label_name, alignment_mode=\"strict\")\n",
    "        if span:\n",
    "            return span\n",
    "    return None\n",
    "\n",
    "def clean_labels(entry, nlp):\n",
    "    \"\"\"\n",
    "    Gibt zurÃ¼ck:\n",
    "      - cleaned_labels: Liste der erfolgreich bereinigten spans\n",
    "      - failures: Liste von Failure-Records mit orig_label + suggestion\n",
    "    \"\"\"\n",
    "    text   = entry.get(\"text\", \"\")\n",
    "    labels = entry.get(\"labels\", [])\n",
    "    doc    = nlp(text)\n",
    "\n",
    "    cleaned_labels = []\n",
    "    failures = []\n",
    "\n",
    "    for label in labels:\n",
    "        start, end, label_name = label[\"start\"], label[\"end\"], label[\"label\"]\n",
    "\n",
    "        # 1) strict\n",
    "        span = doc.char_span(start, end, label=label_name, alignment_mode=\"strict\")\n",
    "        if span:\n",
    "            cleaned_labels.append({\n",
    "                \"start\": span.start_char,\n",
    "                \"end\":   span.end_char,\n",
    "                \"label\": span.label_\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        # 2) single-char expand\n",
    "        span = try_expand_one_char(doc, text, start, end, label_name)\n",
    "        if span:\n",
    "            cleaned_labels.append({\n",
    "                \"start\": span.start_char,\n",
    "                \"end\":   span.end_char,\n",
    "                \"label\": span.label_\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        # 3) token-extend fallback: nur vorschlagen\n",
    "        span_fb = doc.char_span(start, end, label=label_name, alignment_mode=\"expand\")\n",
    "        if span_fb:\n",
    "            suggested = {\n",
    "                \"start\": span_fb.start_char,\n",
    "                \"end\":   span_fb.end_char,\n",
    "                \"text\":  span_fb.text\n",
    "            }\n",
    "        else:\n",
    "            suggested = None\n",
    "\n",
    "        # 4) record failure\n",
    "        failures.append({\n",
    "            \"orig_label\": {\n",
    "                \"start\": start,\n",
    "                \"end\":   end,\n",
    "                \"label\": label_name,\n",
    "                \"text\":  text[start:end]\n",
    "            },\n",
    "            \"suggestion\": suggested\n",
    "        })\n",
    "\n",
    "    return cleaned_labels, failures\n",
    "\n",
    "def process_file(input_path, cleaned_path, failed_path):\n",
    "    # 1) Einlesen\n",
    "    with input_path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    cleaned_data = []\n",
    "    failed_data  = []\n",
    "\n",
    "    # 2) pro Eintrag:\n",
    "    for entry in data:\n",
    "        cleaned_labels, failures = clean_labels(entry, nlp)\n",
    "\n",
    "        if failures:\n",
    "            # Mindestens ein Label ist fehlgeschlagen â†’\n",
    "            # wir speichern die ganze Mail in failed_data:\n",
    "            failed_data.append({\n",
    "                \"file\":       entry.get(\"file\", \"\"),\n",
    "                \"text\":       entry.get(\"text\", \"\"),\n",
    "                \"orig_labels\": entry.get(\"labels\", []),\n",
    "                \"failures\":   failures\n",
    "            })\n",
    "        else:\n",
    "            # Alle Labels sauber â‡’ in cleaned_data\n",
    "            cleaned_data.append({\n",
    "                \"file\":   entry.get(\"file\", \"\"),\n",
    "                \"text\":   entry.get(\"text\", \"\"),\n",
    "                \"labels\": cleaned_labels\n",
    "            })\n",
    "\n",
    "    # 3) Speichern\n",
    "    with cleaned_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(cleaned_data, f, ensure_ascii=False, indent=2)\n",
    "    with failed_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(failed_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    # 4) Reporting auf E-Mail-Ebene\n",
    "    total_emails       = len(data)\n",
    "    total_cleaned      = len(cleaned_data)\n",
    "    total_failed_mail  = len(failed_data)\n",
    "\n",
    "    print(f\"ðŸ“„ Gesamte Mails verarbeitet: {total_emails}\")\n",
    "    print(f\"âœ… VollstÃ¤ndig bereinigt:     {total_cleaned}\")\n",
    "    print(f\"âš ï¸  Mit Fehlern (failed):      {total_failed_mail}\\n\")\n",
    "\n",
    "    if total_failed_mail:\n",
    "        print(\"Beispiele fehlerhafter Mails:\")\n",
    "        for rec in failed_data[:5]:\n",
    "            print(f\" â€¢ Datei {rec['file']!r}:\")\n",
    "            for f in rec[\"failures\"]:\n",
    "                orig = f[\"orig_label\"]\n",
    "                print(f\"    â€“ Orig: '{orig['text']}' @{orig['start']}-{orig['end']} ({orig['label']})\")\n",
    "                if f[\"suggestion\"]:\n",
    "                    s = f[\"suggestion\"]\n",
    "                    print(f\"      â†³ Vorschlag: '{s['text']}' @{s['start']}-{s['end']}\")\n",
    "                else:\n",
    "                    print(\"      â†³ Kein Vorschlag mÃ¶glich\")\n",
    "            print()\n",
    "\n",
    "    print(f\"âœ… Bereinigt-Datei: {cleaned_path.resolve()}\")\n",
    "    print(f\"âœ… Failed- Datei:   {failed_path.resolve()}\")\n",
    "\n",
    "# AusfÃ¼hren\n",
    "process_file(input_path, cleaned_path, failed_path)"
   ],
   "id": "6d40b0a0c2d1b1c6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“„ Gesamte Mails verarbeitet: 14360\n",
      "âœ… VollstÃ¤ndig bereinigt:     13506\n",
      "âš ï¸  Mit Fehlern (failed):      854\n",
      "\n",
      "Beispiele fehlerhafter Mails:\n",
      " â€¢ Datei '16':\n",
      "    â€“ Orig: 'Dipl.-Ing' @121-130 (TITEL)\n",
      "      â†³ Vorschlag: 'ADipl.-Ingesse' @120-134\n",
      "\n",
      " â€¢ Datei '18':\n",
      "    â€“ Orig: 'Aurelia' @340-347 (VORNAME)\n",
      "      â†³ Vorschlag: 'Aurelia_LÃ¶ffler' @340-355\n",
      "    â€“ Orig: 'LÃ¶ffler' @348-355 (NACHNAME)\n",
      "      â†³ Vorschlag: 'Aurelia_LÃ¶ffler' @340-355\n",
      "\n",
      " â€¢ Datei '49':\n",
      "    â€“ Orig: 'Eugen' @205-210 (VORNAME)\n",
      "      â†³ Vorschlag: 'EugenKlemt' @205-215\n",
      "    â€“ Orig: 'Klemt' @210-215 (NACHNAME)\n",
      "      â†³ Vorschlag: 'EugenKlemt' @205-215\n",
      "\n",
      " â€¢ Datei '76':\n",
      "    â€“ Orig: 'Mirjam' @184-190 (VORNAME)\n",
      "      â†³ Vorschlag: 'MirjamDowerg' @184-196\n",
      "    â€“ Orig: 'Dowerg' @190-196 (NACHNAME)\n",
      "      â†³ Vorschlag: 'MirjamDowerg' @184-196\n",
      "\n",
      " â€¢ Datei '79':\n",
      "    â€“ Orig: 'Klapp' @189-194 (NACHNAME)\n",
      "      â†³ Vorschlag: 'Klapp402' @189-197\n",
      "    â€“ Orig: '402 862 472 874' @194-209 (VERTRAGSNUMMER)\n",
      "      â†³ Vorschlag: 'Klapp402 862 472 874' @189-209\n",
      "\n",
      "âœ… Bereinigt-Datei: /Users/timonmartens/Library/CloudStorage/OneDrive-PersoÌˆnlich/Desktop/Veranstaltungen/Data Analytics in Applications/daia-eon/data/synthetic/synthetic_mails_option_b_cleaned_new.json\n",
      "âœ… Failed- Datei:   /Users/timonmartens/Library/CloudStorage/OneDrive-PersoÌˆnlich/Desktop/Veranstaltungen/Data Analytics in Applications/daia-eon/data/synthetic/synthetic_mails_option_b_failed_new.json\n"
     ]
    }
   ],
   "execution_count": 14
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
